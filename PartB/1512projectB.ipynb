{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Adaptive pruning optimization and performance evaluation**"
      ],
      "metadata": {
        "id": "wzOeYODc9rII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Introduce necessary dependencies"
      ],
      "metadata": {
        "id": "0XxoFL949nTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9U4CHq09jfZ",
        "outputId": "9b9e1c3f-8c67-442b-c67c-6bf009d7cd18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (11.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=55a5ad9a20be8aaeb3b65a76c8bc6e8fd07fad9192edcd34670128a07d12baaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=e7359d9415b338bffe4039fcb53bb0c433183107b3656a75092808c9a4be20c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.0.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0Klx50n9Is3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "import torch.profiler as profiler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from copy import deepcopy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Token Saliency computing module"
      ],
      "metadata": {
        "id": "q6-lFQii9t0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenSaliency(nn.Module):\n",
        "    \"\"\"\n",
        "    Compute saliency scores for visual tokens based on their contribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, method=\"norm\"):\n",
        "        super(TokenSaliency, self).__init__()\n",
        "        self.method = method\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tokens: Tensor of shape (B, N, D), where\n",
        "                    B = Batch size,\n",
        "                    N = Number of tokens,\n",
        "                    D = Dimension of each token.\n",
        "        Returns:\n",
        "            saliency_scores: Tensor of shape (B, N), saliency scores for each token.\n",
        "        \"\"\"\n",
        "        if self.method == \"norm\":\n",
        "            saliency_scores = tokens.norm(dim=-1)  # Use L2 norm\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported method: {self.method}\")\n",
        "\n",
        "        return saliency_scores\n"
      ],
      "metadata": {
        "id": "Jf7PMrpt9tmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Adaptive Token pruning module"
      ],
      "metadata": {
        "id": "SsjQ5NgI9xBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaptiveTokenPruning(nn.Module):\n",
        "    def __init__(self, saliency_threshold=0.5):\n",
        "        super(AdaptiveTokenPruning, self).__init__()\n",
        "        self.saliency_threshold = saliency_threshold\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute token saliency and generate a pruning mask.\n",
        "        \"\"\"\n",
        "        saliency_scores = self.compute_saliency(x)\n",
        "        keep_tokens = saliency_scores > self.saliency_threshold\n",
        "        return keep_tokens\n",
        "\n",
        "    def compute_saliency(self, x):\n",
        "        \"\"\"\n",
        "        Compute saliency scores (e.g., L2 norm across embedding dimensions).\n",
        "        \"\"\"\n",
        "        saliency_scores = x.norm(p=2, dim=-1)  # Shape: (batch_size, seq_len)\n",
        "        return saliency_scores\n",
        "\n"
      ],
      "metadata": {
        "id": "4ltpEBEn9ywb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Pruned Transformer Encoder"
      ],
      "metadata": {
        "id": "BE6I4x5d90OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrunedTransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer encoder with token pruning capability.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_layer, num_layers, saliency_threshold=0.5):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([deepcopy(encoder_layer) for _ in range(num_layers)])\n",
        "        self.token_pruning = AdaptiveTokenPruning(saliency_threshold=saliency_threshold)\n",
        "\n",
        "    def forward(self, src):\n",
        "        \"\"\"\n",
        "        Forward pass with token pruning.\n",
        "        Args:\n",
        "            src: Input tensor of shape (batch_size, seq_len, d_model).\n",
        "        Returns:\n",
        "            Output tensor after pruning.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = src.shape\n",
        "        keep_tokens = torch.ones((batch_size, seq_len), device=src.device).bool()  # Initialize with all True\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            # Calculate saliency scores\n",
        "            saliency = self.token_pruning.compute_saliency(src)\n",
        "\n",
        "            # Update keep_tokens\n",
        "            new_keep_tokens = (saliency > self.token_pruning.saliency_threshold)\n",
        "            keep_tokens = keep_tokens & new_keep_tokens  # Retain the accumulated crop state\n",
        "\n",
        "            # Dynamically crop the input tensor\n",
        "            pruned_src = []\n",
        "            pruned_keep_tokens = []\n",
        "\n",
        "            for batch_idx in range(batch_size):\n",
        "                active_token_indices = keep_tokens[batch_idx].nonzero(as_tuple=True)[0]\n",
        "                pruned_src.append(src[batch_idx, active_token_indices])\n",
        "                pruned_keep_tokens.append(keep_tokens[batch_idx, active_token_indices])\n",
        "\n",
        "            # Update src and keep_tokens with the trimmed tensor\n",
        "            src = torch.nn.utils.rnn.pad_sequence(pruned_src, batch_first=True)\n",
        "            keep_tokens = torch.nn.utils.rnn.pad_sequence(pruned_keep_tokens, batch_first=True)\n",
        "\n",
        "            # Print debugging information\n",
        "            print(f\"Layer {i}: Active tokens per batch = {[len(t) for t in pruned_src]}\")\n",
        "\n",
        "            # Pass the clipped tensor to the next layer\n",
        "            src = layer(src)\n",
        "\n",
        "        return src\n",
        "\n"
      ],
      "metadata": {
        "id": "2TVs-S5O91r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. FLOPs evaluation tool"
      ],
      "metadata": {
        "id": "2_S1DyIq93o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "\n",
        "def calculate_dynamic_flops(model, x, keep_tokens):\n",
        "    \"\"\"\n",
        "    Calculate FLOPs dynamically based on active tokens.\n",
        "    Args:\n",
        "        model: The pruned Transformer model.\n",
        "        x: Input tensor of shape (batch_size, seq_len, d_model).\n",
        "        keep_tokens: Boolean tensor indicating active tokens for the pruned model.\n",
        "    \"\"\"\n",
        "    #  Get the maximum number of active tokens\n",
        "    active_tokens = keep_tokens.sum(dim=1).max().item()\n",
        "    x = x[:, :active_tokens, :]  # Crop to active Token\n",
        "    flops = FlopCountAnalysis(model, x)\n",
        "    print(flop_count_table(flops))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YJ5rD84T95p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Memory usage evaluation tool"
      ],
      "metadata": {
        "id": "7Itdt7Px-AmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def profile_memory_and_time_safe(model, input_tensor):\n",
        "    \"\"\"\n",
        "    Profile memory and time for the given model and input.\n",
        "    Args:\n",
        "        model: PyTorch model to profile.\n",
        "        input_tensor: Tensor input to pass through the model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with torch.profiler.profile(\n",
        "            activities=[\n",
        "                torch.profiler.ProfilerActivity.CPU,\n",
        "                torch.profiler.ProfilerActivity.CUDA,\n",
        "            ],\n",
        "            record_shapes=True,\n",
        "            profile_memory=True,\n",
        "            with_stack=False,  # Disable stack tracing to reduce possible conflicts\n",
        "        ) as prof:\n",
        "            model(input_tensor)  # Perform model forward propagation\n",
        "        print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Profiler failed: {e}\")\n"
      ],
      "metadata": {
        "id": "RBcX4HVe97vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Model accuracy evaluation"
      ],
      "metadata": {
        "id": "RhZFO-7F-E5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_accuracy(model, train_data, train_labels, test_data, test_labels):\n",
        "    \"\"\"\n",
        "    Train and evaluate model accuracy on a toy dataset.\n",
        "    Args:\n",
        "        model: PyTorch model to evaluate.\n",
        "        train_data, train_labels, test_data, test_labels: Dataset tensors.\n",
        "    \"\"\"\n",
        "    # Make sure the shape of the label is 1D\n",
        "    train_labels = train_labels.squeeze()\n",
        "    test_labels = test_labels.squeeze()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Dataset and DataLoader\n",
        "    train_dataset = TensorDataset(train_data, train_labels)\n",
        "    test_dataset = TensorDataset(test_data, test_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "    # Optimizer and Loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(5):\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.mean(dim=1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluation loop\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.mean(dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "fTAx9kA7-BhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "\n",
        "def evaluate_pruned_model(baseline_model, pruned_model, test_data):\n",
        "    \"\"\"\n",
        "    Compare baseline and pruned models in terms of FLOPs and active token efficiency.\n",
        "    Args:\n",
        "        baseline_model: The baseline Transformer model.\n",
        "        pruned_model: The pruned Transformer model.\n",
        "        test_data: Sample input tensor for efficiency evaluation.\n",
        "    \"\"\"\n",
        "    print(\"=== Baseline Model Efficiency ===\")\n",
        "    flops_baseline = FlopCountAnalysis(baseline_model, test_data)\n",
        "    print(flop_count_table(flops_baseline))\n",
        "\n",
        "    print(\"\\n=== Pruned Model Efficiency ===\")\n",
        "    # Assuming the PrunedTransformerEncoder dynamically prunes tokens\n",
        "    with torch.no_grad():\n",
        "        pruned_outputs = pruned_model[0](test_data)  # Get the intermediate result of PrunedTransformer\n",
        "        active_tokens = pruned_outputs.shape[1]  # The number of valid tokens remaining\n",
        "        flops_pruned = FlopCountAnalysis(pruned_model, test_data[:, :active_tokens, :])\n",
        "        print(flop_count_table(flops_pruned))\n"
      ],
      "metadata": {
        "id": "r92rw3JYLiYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Prepare the data set"
      ],
      "metadata": {
        "id": "obWNnHLK-qUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data():\n",
        "    \"\"\"\n",
        "    Prepare simulated toy dataset for training and testing.\n",
        "    Returns:\n",
        "        train_data, train_labels, test_data, test_labels\n",
        "    \"\"\"\n",
        "    train_data = torch.rand(1000, 128, 512).cuda()  # 1000 samples, 128 tokens, 512 dimensions\n",
        "    train_labels = torch.randint(0, 2, (1000,), dtype=torch.long).cuda()  # Make sure it's a 1D long integral tensor\n",
        "    test_labels = torch.randint(0, 2, (200,), dtype=torch.long).cuda()\n",
        "    test_data = torch.rand(200, 128, 512).cuda()  # 200 samples for testing\n",
        "    return train_data, train_labels, test_data, test_labels\n"
      ],
      "metadata": {
        "id": "nNjSCIOd-DR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels, test_data, test_labels = prepare_data()\n",
        "print(train_data.shape, train_labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWy8RS4u-t-j",
        "outputId": "f9da2f7b-7299-4b8c-aad9-824f04fef194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 128, 512]) torch.Size([1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Define the model"
      ],
      "metadata": {
        "id": "xZ4hp0vM-401"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep the SimpleClassifierHead class\n",
        "class SimpleClassifierHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple classification head for transformer output.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(SimpleClassifierHead, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "# TransformerEncoderLayerWithPruning class\n",
        "class TransformerEncoderLayerWithPruning(nn.TransformerEncoderLayer):\n",
        "    \"\"\"\n",
        "    A customized TransformerEncoderLayer that supports dynamic token skipping.\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None, keep_tokens=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Input tensor of shape (batch_size, seq_len, d_model).\n",
        "            keep_tokens: Boolean tensor of shape (batch_size, seq_len).\n",
        "        \"\"\"\n",
        "        if keep_tokens is not None:\n",
        "            # Dynamically crop the tensor shape, keeping only tokens marked True\n",
        "            batch_size, seq_len, d_model = src.shape\n",
        "            active_indices = keep_tokens.nonzero(as_tuple=True)  # Gets the index of active tokens\n",
        "            max_active_tokens = keep_tokens.sum(dim=1).max().item()  # Maximum number of active tokens\n",
        "            pruned_src = torch.zeros(batch_size, max_active_tokens, d_model, device=src.device)\n",
        "\n",
        "            for batch_idx in range(batch_size):\n",
        "                active_token_indices = keep_tokens[batch_idx].nonzero(as_tuple=True)[0]\n",
        "                pruned_src[batch_idx, :len(active_token_indices)] = src[batch_idx, active_token_indices]\n",
        "\n",
        "            src = pruned_src  # Update to the clipped tensor\n",
        "\n",
        "        # A forward method that passes the trimmed tensor to the parent class\n",
        "        return super().forward(src, src_mask, src_key_padding_mask)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create_models function\n",
        "def create_models():\n",
        "    \"\"\"\n",
        "    Create baseline and pruned Transformer models, each with a classification head.\n",
        "    Returns:\n",
        "        baseline_model, pruned_model\n",
        "    \"\"\"\n",
        "    num_classes = 2  # dichotomy\n",
        "\n",
        "    # Baseline model\n",
        "    baseline_encoder = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "    baseline_transformer = nn.TransformerEncoder(baseline_encoder, num_layers=2).cuda()\n",
        "    baseline_model = nn.Sequential(\n",
        "        baseline_transformer,\n",
        "        SimpleClassifierHead(input_dim=512, num_classes=num_classes).cuda()\n",
        "    )\n",
        "\n",
        "    # Pruned model\n",
        "    pruned_encoder = TransformerEncoderLayerWithPruning(d_model=512, nhead=8)\n",
        "    pruned_transformer = PrunedTransformerEncoder(pruned_encoder, num_layers=2, saliency_threshold=13.0).cuda()\n",
        "    pruned_model = nn.Sequential(\n",
        "        pruned_transformer,\n",
        "        SimpleClassifierHead(input_dim=512, num_classes=num_classes).cuda()\n",
        "    )\n",
        "\n",
        "    return baseline_model, pruned_model\n",
        "\n"
      ],
      "metadata": {
        "id": "DhaStRaq-6sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model, pruned_model = create_models()\n",
        "print(baseline_model)\n",
        "print(pruned_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWmvKyhq-8VH",
        "outputId": "7cfbb3c7-20ab-491a-d31c-03f114bcc278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (1): SimpleClassifierHead(\n",
            "    (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "Sequential(\n",
            "  (0): PrunedTransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x TransformerEncoderLayerWithPruning(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (token_pruning): AdaptiveTokenPruning()\n",
            "  )\n",
            "  (1): SimpleClassifierHead(\n",
            "    (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.rand(16, 128, 512).cuda()\n"
      ],
      "metadata": {
        "id": "3JJRviO4mgPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_outputs = pruned_model[0](input_tensor)\n",
        "print(f\"Output shape after pruning: {pruned_outputs.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTvdmiAcmhJV",
        "outputId": "509881a2-c8be-4da2-b70b-a8390ad077d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0: Active tokens per batch = [79, 80, 65, 79, 86, 76, 80, 70, 82, 79, 68, 71, 73, 78, 83, 69]\n",
            "Layer 1: Active tokens per batch = [79, 80, 65, 79, 86, 76, 80, 70, 82, 79, 68, 71, 73, 78, 83, 69]\n",
            "Output shape after pruning: torch.Size([16, 86, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saliency_scores = pruned_model[0].token_pruning.compute_saliency(input_tensor)\n",
        "print(f\"Saliency scores range: {saliency_scores.min().item()} - {saliency_scores.max().item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYx7EjnvZlWv",
        "outputId": "7166f968-f813-40f0-92d6-879567c8ee6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saliency scores range: 12.195259094238281 - 13.956788063049316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keep_tokens = pruned_model[0].token_pruning.compute_saliency(input_tensor) > pruned_model[0].token_pruning.saliency_threshold\n",
        "print(f\"Keep tokens mask (sample batch): {keep_tokens[0].cpu().numpy()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8wOpCUQaeIn",
        "outputId": "f500292d-8473-4851-d116-948ee058ac7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keep tokens mask (sample batch): [False  True False  True False False  True  True  True  True  True False\n",
            "  True  True  True  True False  True  True  True False False  True  True\n",
            " False False False  True  True  True  True False  True  True False  True\n",
            " False  True False  True  True  True False False  True False  True  True\n",
            " False False  True  True  True False  True False  True False False False\n",
            "  True  True  True False  True  True  True False  True  True  True  True\n",
            "  True  True  True  True False  True  True False  True False  True False\n",
            " False False  True False False  True False  True False  True  True False\n",
            "  True  True  True  True False  True  True  True False False False  True\n",
            "  True  True  True False  True  True  True  True  True False False False\n",
            "  True  True False  True False False  True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Evaluate model accuracy"
      ],
      "metadata": {
        "id": "mwZNn8YY_mxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models_accuracy(baseline_model, pruned_model, train_data, train_labels, test_data, test_labels):\n",
        "    \"\"\"\n",
        "    Compare accuracy of baseline and pruned models.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Baseline Model Accuracy ===\")\n",
        "    evaluate_model_accuracy(baseline_model, train_data, train_labels, test_data, test_labels)\n",
        "\n",
        "    print(\"\\n=== Pruned Model Accuracy ===\")\n",
        "    evaluate_model_accuracy(pruned_model, train_data, train_labels, test_data, test_labels)\n"
      ],
      "metadata": {
        "id": "MbHeFI2G_neM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models_accuracy(baseline_model, pruned_model, train_data, train_labels, test_data, test_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-JnKU8f_4ZF",
        "outputId": "9d692b2c-abd5-49c2-8385-ced675998b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Baseline Model Accuracy ===\n",
            "Accuracy: 47.50%\n",
            "\n",
            "=== Pruned Model Accuracy ===\n",
            "Layer 0: Active tokens per batch = [71, 84, 77, 70, 71, 82, 64, 80, 75, 74, 71, 76, 80, 77, 75, 82]\n",
            "Layer 1: Active tokens per batch = [71, 84, 77, 70, 71, 82, 64, 80, 75, 74, 71, 76, 80, 77, 75, 82]\n",
            "Layer 0: Active tokens per batch = [82, 75, 76, 71, 71, 75, 79, 76, 76, 77, 80, 79, 78, 71, 91, 80]\n",
            "Layer 1: Active tokens per batch = [82, 75, 76, 71, 71, 75, 79, 76, 76, 77, 80, 79, 78, 71, 91, 80]\n",
            "Layer 0: Active tokens per batch = [78, 77, 69, 72, 71, 70, 83, 74, 81, 77, 67, 73, 75, 71, 78, 73]\n",
            "Layer 1: Active tokens per batch = [78, 77, 69, 72, 71, 70, 83, 74, 81, 77, 67, 73, 75, 71, 78, 73]\n",
            "Layer 0: Active tokens per batch = [70, 68, 78, 78, 82, 88, 75, 69, 79, 74, 79, 70, 71, 84, 79, 67]\n",
            "Layer 1: Active tokens per batch = [70, 68, 78, 78, 82, 88, 75, 69, 79, 74, 79, 70, 71, 84, 79, 67]\n",
            "Layer 0: Active tokens per batch = [65, 84, 70, 79, 75, 76, 79, 72, 75, 74, 78, 81, 62, 82, 74, 75]\n",
            "Layer 1: Active tokens per batch = [65, 84, 70, 79, 75, 76, 79, 72, 75, 74, 78, 81, 62, 82, 74, 75]\n",
            "Layer 0: Active tokens per batch = [76, 85, 91, 71, 72, 80, 67, 69, 75, 75, 77, 78, 70, 84, 69, 74]\n",
            "Layer 1: Active tokens per batch = [76, 85, 91, 71, 72, 80, 67, 69, 75, 75, 77, 78, 70, 84, 69, 74]\n",
            "Layer 0: Active tokens per batch = [72, 70, 79, 79, 80, 85, 67, 79, 84, 71, 84, 61, 75, 83, 78, 76]\n",
            "Layer 1: Active tokens per batch = [72, 70, 79, 79, 80, 85, 67, 79, 84, 71, 84, 61, 75, 83, 78, 76]\n",
            "Layer 0: Active tokens per batch = [84, 76, 78, 73, 75, 86, 70, 75, 78, 85, 80, 68, 74, 77, 71, 78]\n",
            "Layer 1: Active tokens per batch = [84, 76, 78, 73, 75, 86, 70, 75, 78, 85, 80, 68, 74, 77, 71, 78]\n",
            "Layer 0: Active tokens per batch = [68, 79, 77, 70, 68, 75, 82, 77, 71, 77, 81, 76, 67, 81, 77, 73]\n",
            "Layer 1: Active tokens per batch = [68, 79, 77, 70, 68, 75, 82, 77, 71, 77, 81, 76, 67, 81, 77, 73]\n",
            "Layer 0: Active tokens per batch = [80, 66, 87, 72, 77, 80, 75, 66, 72, 70, 73, 76, 81, 78, 70, 83]\n",
            "Layer 1: Active tokens per batch = [80, 66, 87, 72, 77, 80, 75, 66, 72, 70, 73, 76, 81, 78, 70, 83]\n",
            "Layer 0: Active tokens per batch = [67, 78, 76, 81, 72, 70, 82, 80, 75, 76, 78, 78, 77, 77, 67, 74]\n",
            "Layer 1: Active tokens per batch = [67, 78, 76, 81, 72, 70, 82, 80, 75, 76, 78, 78, 77, 77, 67, 74]\n",
            "Layer 0: Active tokens per batch = [73, 88, 78, 81, 86, 77, 83, 73, 74, 82, 76, 73, 84, 79, 71, 76]\n",
            "Layer 1: Active tokens per batch = [73, 88, 78, 81, 86, 77, 83, 73, 74, 82, 76, 73, 84, 79, 71, 76]\n",
            "Layer 0: Active tokens per batch = [72, 74, 76, 74, 78, 77, 78, 69, 87, 73, 82, 80, 67, 79, 79, 85]\n",
            "Layer 1: Active tokens per batch = [72, 74, 76, 74, 78, 77, 78, 69, 87, 73, 82, 80, 67, 79, 79, 85]\n",
            "Layer 0: Active tokens per batch = [78, 80, 83, 80, 79, 77, 70, 74, 75, 70, 79, 74, 77, 80, 67, 79]\n",
            "Layer 1: Active tokens per batch = [78, 80, 83, 80, 79, 77, 70, 74, 75, 70, 79, 74, 77, 80, 67, 79]\n",
            "Layer 0: Active tokens per batch = [68, 81, 80, 74, 78, 75, 72, 77, 86, 69, 72, 69, 73, 72, 78, 73]\n",
            "Layer 1: Active tokens per batch = [68, 81, 80, 74, 78, 75, 72, 77, 86, 69, 72, 69, 73, 72, 78, 73]\n",
            "Layer 0: Active tokens per batch = [83, 81, 72, 80, 72, 80, 77, 76, 72, 76, 78, 75, 84, 69, 87, 77]\n",
            "Layer 1: Active tokens per batch = [83, 81, 72, 80, 72, 80, 77, 76, 72, 76, 78, 75, 84, 69, 87, 77]\n",
            "Layer 0: Active tokens per batch = [73, 85, 78, 82, 81, 80, 71, 71, 79, 78, 68, 85, 81, 74, 87, 70]\n",
            "Layer 1: Active tokens per batch = [73, 85, 78, 82, 81, 80, 71, 71, 79, 78, 68, 85, 81, 74, 87, 70]\n",
            "Layer 0: Active tokens per batch = [86, 76, 82, 79, 81, 80, 77, 75, 74, 80, 69, 82, 76, 78, 72, 77]\n",
            "Layer 1: Active tokens per batch = [86, 76, 82, 79, 81, 80, 77, 75, 74, 80, 69, 82, 76, 78, 72, 77]\n",
            "Layer 0: Active tokens per batch = [78, 79, 73, 79, 67, 74, 82, 77, 86, 75, 63, 85, 79, 65, 81, 78]\n",
            "Layer 1: Active tokens per batch = [78, 79, 73, 79, 67, 74, 82, 77, 86, 75, 63, 85, 79, 65, 81, 78]\n",
            "Layer 0: Active tokens per batch = [75, 76, 69, 81, 72, 69, 84, 71, 80, 83, 71, 80, 77, 76, 70, 80]\n",
            "Layer 1: Active tokens per batch = [75, 76, 69, 81, 72, 69, 84, 71, 80, 83, 71, 80, 77, 76, 70, 80]\n",
            "Layer 0: Active tokens per batch = [79, 73, 71, 85, 84, 72, 79, 71, 75, 86, 74, 73, 69, 72, 76, 78]\n",
            "Layer 1: Active tokens per batch = [79, 73, 71, 85, 84, 72, 79, 71, 75, 86, 74, 73, 69, 72, 76, 78]\n",
            "Layer 0: Active tokens per batch = [66, 70, 70, 76, 72, 81, 69, 70, 68, 73, 79, 75, 80, 67, 79, 75]\n",
            "Layer 1: Active tokens per batch = [66, 70, 70, 76, 72, 81, 69, 70, 68, 73, 79, 75, 80, 67, 79, 75]\n",
            "Layer 0: Active tokens per batch = [72, 71, 85, 83, 81, 74, 86, 77, 69, 76, 78, 86, 84, 80, 84, 79]\n",
            "Layer 1: Active tokens per batch = [72, 71, 85, 83, 81, 74, 86, 77, 69, 76, 78, 86, 84, 80, 84, 79]\n",
            "Layer 0: Active tokens per batch = [79, 79, 79, 74, 73, 82, 81, 79, 76, 80, 81, 74, 69, 80, 64, 82]\n",
            "Layer 1: Active tokens per batch = [79, 79, 79, 74, 73, 82, 81, 79, 76, 80, 81, 74, 69, 80, 64, 82]\n",
            "Layer 0: Active tokens per batch = [68, 77, 82, 74, 80, 77, 64, 70, 76, 79, 73, 71, 75, 74, 76, 68]\n",
            "Layer 1: Active tokens per batch = [68, 77, 82, 74, 80, 77, 64, 70, 76, 79, 73, 71, 75, 74, 76, 68]\n",
            "Layer 0: Active tokens per batch = [76, 74, 75, 84, 69, 84, 65, 76, 74, 73, 68, 64, 81, 78, 80, 75]\n",
            "Layer 1: Active tokens per batch = [76, 74, 75, 84, 69, 84, 65, 76, 74, 73, 68, 64, 81, 78, 80, 75]\n",
            "Layer 0: Active tokens per batch = [74, 68, 72, 78, 80, 79, 77, 82, 75, 78, 85, 68, 84, 80, 94, 80]\n",
            "Layer 1: Active tokens per batch = [74, 68, 72, 78, 80, 79, 77, 82, 75, 78, 85, 68, 84, 80, 94, 80]\n",
            "Layer 0: Active tokens per batch = [78, 74, 76, 80, 73, 67, 71, 71, 82, 72, 79, 76, 74, 75, 77, 84]\n",
            "Layer 1: Active tokens per batch = [78, 74, 76, 80, 73, 67, 71, 71, 82, 72, 79, 76, 74, 75, 77, 84]\n",
            "Layer 0: Active tokens per batch = [78, 76, 82, 62, 76, 79, 80, 74, 80, 74, 83, 71, 77, 86, 70, 69]\n",
            "Layer 1: Active tokens per batch = [78, 76, 82, 62, 76, 79, 80, 74, 80, 74, 83, 71, 77, 86, 70, 69]\n",
            "Layer 0: Active tokens per batch = [77, 83, 71, 71, 84, 68, 76, 73, 79, 64, 74, 77, 78, 79, 75, 80]\n",
            "Layer 1: Active tokens per batch = [77, 83, 71, 71, 84, 68, 76, 73, 79, 64, 74, 77, 78, 79, 75, 80]\n",
            "Layer 0: Active tokens per batch = [85, 77, 79, 69, 67, 82, 83, 78, 77, 77, 82, 73, 82, 71, 70, 86]\n",
            "Layer 1: Active tokens per batch = [85, 77, 79, 69, 67, 82, 83, 78, 77, 77, 82, 73, 82, 71, 70, 86]\n",
            "Layer 0: Active tokens per batch = [78, 80, 79, 73, 76, 77, 74, 84, 82, 85, 64, 66, 70, 73, 81, 75]\n",
            "Layer 1: Active tokens per batch = [78, 80, 79, 73, 76, 77, 74, 84, 82, 85, 64, 66, 70, 73, 81, 75]\n",
            "Layer 0: Active tokens per batch = [73, 88, 71, 78, 75, 67, 77, 77, 74, 70, 70, 81, 72, 71, 78, 83]\n",
            "Layer 1: Active tokens per batch = [73, 88, 71, 78, 75, 67, 77, 77, 74, 70, 70, 81, 72, 71, 78, 83]\n",
            "Layer 0: Active tokens per batch = [70, 89, 76, 63, 77, 82, 80, 76, 71, 78, 76, 66, 80, 81, 80, 81]\n",
            "Layer 1: Active tokens per batch = [70, 89, 76, 63, 77, 82, 80, 76, 71, 78, 76, 66, 80, 81, 80, 81]\n",
            "Layer 0: Active tokens per batch = [77, 77, 70, 72, 69, 79, 74, 80, 74, 73, 72, 84, 75, 82, 72, 80]\n",
            "Layer 1: Active tokens per batch = [77, 77, 70, 72, 69, 79, 74, 80, 74, 73, 72, 84, 75, 82, 72, 80]\n",
            "Layer 0: Active tokens per batch = [85, 81, 79, 77, 88, 71, 72, 82, 70, 73, 86, 74, 68, 76, 73, 91]\n",
            "Layer 1: Active tokens per batch = [85, 81, 79, 77, 88, 71, 72, 82, 70, 73, 86, 74, 68, 76, 73, 91]\n",
            "Layer 0: Active tokens per batch = [82, 80, 79, 78, 72, 78, 76, 80, 78, 77, 79, 74, 85, 77, 77, 66]\n",
            "Layer 1: Active tokens per batch = [82, 80, 79, 78, 72, 78, 76, 80, 78, 77, 79, 74, 85, 77, 77, 66]\n",
            "Layer 0: Active tokens per batch = [78, 74, 86, 64, 76, 72, 74, 75, 72, 78, 71, 82, 74, 73, 75, 69]\n",
            "Layer 1: Active tokens per batch = [78, 74, 86, 64, 76, 72, 74, 75, 72, 78, 71, 82, 74, 73, 75, 69]\n",
            "Layer 0: Active tokens per batch = [73, 81, 76, 78, 72, 81, 74, 77, 75, 79, 77, 75, 72, 72, 80, 86]\n",
            "Layer 1: Active tokens per batch = [73, 81, 76, 78, 72, 81, 74, 77, 75, 79, 77, 75, 72, 72, 80, 86]\n",
            "Layer 0: Active tokens per batch = [69, 76, 71, 80, 78, 69, 67, 74, 71, 70, 75, 70, 82, 75, 80, 82]\n",
            "Layer 1: Active tokens per batch = [69, 76, 71, 80, 78, 69, 67, 74, 71, 70, 75, 70, 82, 75, 80, 82]\n",
            "Layer 0: Active tokens per batch = [74, 74, 77, 78, 81, 82, 86, 76, 75, 86, 64, 78, 68, 75, 67, 78]\n",
            "Layer 1: Active tokens per batch = [74, 74, 77, 78, 81, 82, 86, 76, 75, 86, 64, 78, 68, 75, 67, 78]\n",
            "Layer 0: Active tokens per batch = [80, 77, 80, 78, 80, 75, 85, 79, 81, 75, 74, 66, 85, 89, 72, 79]\n",
            "Layer 1: Active tokens per batch = [80, 77, 80, 78, 80, 75, 85, 79, 81, 75, 74, 66, 85, 89, 72, 79]\n",
            "Layer 0: Active tokens per batch = [82, 79, 75, 75, 87, 82, 85, 78, 64, 96, 76, 79, 74, 71, 79, 71]\n",
            "Layer 1: Active tokens per batch = [82, 79, 75, 75, 87, 82, 85, 78, 64, 96, 76, 79, 74, 71, 79, 71]\n",
            "Layer 0: Active tokens per batch = [74, 76, 76, 74, 70, 73, 83, 74, 82, 76, 81, 87, 67, 78, 80, 71]\n",
            "Layer 1: Active tokens per batch = [74, 76, 76, 74, 70, 73, 83, 74, 82, 76, 81, 87, 67, 78, 80, 71]\n",
            "Layer 0: Active tokens per batch = [77, 79, 84, 73, 71, 73, 69, 80, 72, 76, 78, 74, 77, 69, 73, 75]\n",
            "Layer 1: Active tokens per batch = [77, 79, 84, 73, 71, 73, 69, 80, 72, 76, 78, 74, 77, 69, 73, 75]\n",
            "Layer 0: Active tokens per batch = [78, 80, 79, 73, 81, 84, 72, 81, 71, 76, 80, 66, 82, 79, 71, 70]\n",
            "Layer 1: Active tokens per batch = [78, 80, 79, 73, 81, 84, 72, 81, 71, 76, 80, 66, 82, 79, 71, 70]\n",
            "Layer 0: Active tokens per batch = [79, 73, 82, 85, 69, 69, 80, 77, 67, 77, 68, 77, 87, 80, 80, 81]\n",
            "Layer 1: Active tokens per batch = [79, 73, 82, 85, 69, 69, 80, 77, 67, 77, 68, 77, 87, 80, 80, 81]\n",
            "Layer 0: Active tokens per batch = [68, 75, 71, 75, 74, 79, 83, 79, 69, 76, 66, 77, 81, 78, 75, 78]\n",
            "Layer 1: Active tokens per batch = [68, 75, 71, 75, 74, 79, 83, 79, 69, 76, 66, 77, 81, 78, 75, 78]\n",
            "Layer 0: Active tokens per batch = [72, 75, 78, 82, 85, 84, 74, 69, 77, 79, 76, 65, 82, 70, 74, 71]\n",
            "Layer 1: Active tokens per batch = [72, 75, 78, 82, 85, 84, 74, 69, 77, 79, 76, 65, 82, 70, 74, 71]\n",
            "Layer 0: Active tokens per batch = [67, 80, 67, 76, 75, 78, 75, 81, 76, 79, 73, 78, 70, 79, 72, 76]\n",
            "Layer 1: Active tokens per batch = [67, 80, 67, 76, 75, 78, 75, 81, 76, 79, 73, 78, 70, 79, 72, 76]\n",
            "Layer 0: Active tokens per batch = [73, 77, 80, 82, 82, 84, 82, 75, 73, 77, 79, 73, 75, 74, 83, 77]\n",
            "Layer 1: Active tokens per batch = [73, 77, 80, 82, 82, 84, 82, 75, 73, 77, 79, 73, 75, 74, 83, 77]\n",
            "Layer 0: Active tokens per batch = [78, 87, 76, 73, 79, 73, 74, 66, 66, 71, 68, 76, 72, 81, 75, 78]\n",
            "Layer 1: Active tokens per batch = [78, 87, 76, 73, 79, 73, 74, 66, 66, 71, 68, 76, 72, 81, 75, 78]\n",
            "Layer 0: Active tokens per batch = [74, 76, 75, 82, 74, 72, 73, 79, 82, 70, 77, 74, 65, 78, 75, 75]\n",
            "Layer 1: Active tokens per batch = [74, 76, 75, 82, 74, 72, 73, 79, 82, 70, 77, 74, 65, 78, 75, 75]\n",
            "Layer 0: Active tokens per batch = [79, 73, 69, 85, 81, 73, 77, 73, 78, 71, 76, 81, 75, 74, 85, 69]\n",
            "Layer 1: Active tokens per batch = [79, 73, 69, 85, 81, 73, 77, 73, 78, 71, 76, 81, 75, 74, 85, 69]\n",
            "Layer 0: Active tokens per batch = [70, 69, 86, 79, 72, 72, 82, 74, 83, 72, 83, 75, 90, 78, 77, 73]\n",
            "Layer 1: Active tokens per batch = [70, 69, 86, 79, 72, 72, 82, 74, 83, 72, 83, 75, 90, 78, 77, 73]\n",
            "Layer 0: Active tokens per batch = [65, 78, 72, 67, 74, 77, 87, 68, 76, 84, 69, 74, 75, 68, 79, 75]\n",
            "Layer 1: Active tokens per batch = [65, 78, 72, 67, 74, 77, 87, 68, 76, 84, 69, 74, 75, 68, 79, 75]\n",
            "Layer 0: Active tokens per batch = [77, 73, 86, 67, 78, 71, 76, 76, 70, 74, 72, 74, 77, 77, 74, 82]\n",
            "Layer 1: Active tokens per batch = [77, 73, 86, 67, 78, 71, 76, 76, 70, 74, 72, 74, 77, 77, 74, 82]\n",
            "Layer 0: Active tokens per batch = [70, 71, 80, 82, 70, 76, 74, 74, 78, 63, 74, 76, 72, 71, 84, 68]\n",
            "Layer 1: Active tokens per batch = [70, 71, 80, 82, 70, 76, 74, 74, 78, 63, 74, 76, 72, 71, 84, 68]\n",
            "Layer 0: Active tokens per batch = [73, 67, 83, 77, 80, 81, 78, 83, 83, 82, 82, 78, 87, 68, 56, 71]\n",
            "Layer 1: Active tokens per batch = [73, 67, 83, 77, 80, 81, 78, 83, 83, 82, 82, 78, 87, 68, 56, 71]\n",
            "Layer 0: Active tokens per batch = [85, 75, 71, 79, 78, 73, 85, 71, 72, 84, 78, 75, 82, 81, 77, 67]\n",
            "Layer 1: Active tokens per batch = [85, 75, 71, 79, 78, 73, 85, 71, 72, 84, 78, 75, 82, 81, 77, 67]\n",
            "Layer 0: Active tokens per batch = [66, 74, 83, 69, 75, 77, 64, 77, 79, 78, 78, 81, 70, 79, 83, 69]\n",
            "Layer 1: Active tokens per batch = [66, 74, 83, 69, 75, 77, 64, 77, 79, 78, 78, 81, 70, 79, 83, 69]\n",
            "Layer 0: Active tokens per batch = [73, 76, 80, 78, 80, 78, 79, 79, 86, 73, 81, 72, 83, 68, 74, 77]\n",
            "Layer 1: Active tokens per batch = [73, 76, 80, 78, 80, 78, 79, 79, 86, 73, 81, 72, 83, 68, 74, 77]\n",
            "Layer 0: Active tokens per batch = [75, 86, 68, 86, 76, 84, 81, 65]\n",
            "Layer 1: Active tokens per batch = [75, 86, 68, 86, 76, 84, 81, 65]\n",
            "Layer 0: Active tokens per batch = [81, 71, 71, 66, 71, 75, 75, 78, 78, 77, 75, 81, 74, 76, 86, 82]\n",
            "Layer 1: Active tokens per batch = [81, 71, 71, 66, 71, 75, 75, 78, 78, 77, 75, 81, 74, 76, 86, 82]\n",
            "Layer 0: Active tokens per batch = [78, 78, 74, 74, 74, 82, 75, 73, 69, 82, 82, 79, 81, 80, 76, 79]\n",
            "Layer 1: Active tokens per batch = [78, 78, 74, 74, 74, 82, 75, 73, 69, 82, 82, 79, 81, 80, 76, 79]\n",
            "Layer 0: Active tokens per batch = [70, 78, 78, 70, 84, 82, 86, 79, 74, 72, 81, 69, 77, 81, 82, 70]\n",
            "Layer 1: Active tokens per batch = [70, 78, 78, 70, 84, 82, 86, 79, 74, 72, 81, 69, 77, 81, 82, 70]\n",
            "Layer 0: Active tokens per batch = [70, 81, 66, 80, 73, 75, 74, 71, 79, 77, 72, 87, 79, 77, 71, 68]\n",
            "Layer 1: Active tokens per batch = [70, 81, 66, 80, 73, 75, 74, 71, 79, 77, 72, 87, 79, 77, 71, 68]\n",
            "Layer 0: Active tokens per batch = [73, 78, 64, 70, 81, 69, 81, 73, 76, 73, 89, 80, 71, 81, 73, 80]\n",
            "Layer 1: Active tokens per batch = [73, 78, 64, 70, 81, 69, 81, 73, 76, 73, 89, 80, 71, 81, 73, 80]\n",
            "Layer 0: Active tokens per batch = [70, 68, 65, 71, 69, 74, 72, 81, 74, 83, 71, 80, 82, 75, 70, 71]\n",
            "Layer 1: Active tokens per batch = [70, 68, 65, 71, 69, 74, 72, 81, 74, 83, 71, 80, 82, 75, 70, 71]\n",
            "Layer 0: Active tokens per batch = [71, 72, 80, 73, 68, 82, 80, 79, 76, 84, 64, 80, 75, 85, 81, 85]\n",
            "Layer 1: Active tokens per batch = [71, 72, 80, 73, 68, 82, 80, 79, 76, 84, 64, 80, 75, 85, 81, 85]\n",
            "Layer 0: Active tokens per batch = [75, 81, 73, 80, 79, 77, 79, 81, 83, 86, 74, 76, 76, 72, 71, 78]\n",
            "Layer 1: Active tokens per batch = [75, 81, 73, 80, 79, 77, 79, 81, 83, 86, 74, 76, 76, 72, 71, 78]\n",
            "Layer 0: Active tokens per batch = [84, 70, 84, 83, 81, 87, 69, 74, 69, 68, 73, 91, 80, 75, 74, 75]\n",
            "Layer 1: Active tokens per batch = [84, 70, 84, 83, 81, 87, 69, 74, 69, 68, 73, 91, 80, 75, 74, 75]\n",
            "Layer 0: Active tokens per batch = [85, 68, 70, 78, 72, 77, 77, 75, 68, 76, 79, 81, 80, 79, 73, 81]\n",
            "Layer 1: Active tokens per batch = [85, 68, 70, 78, 72, 77, 77, 75, 68, 76, 79, 81, 80, 79, 73, 81]\n",
            "Layer 0: Active tokens per batch = [80, 78, 75, 81, 76, 78, 79, 78, 69, 79, 76, 69, 68, 80, 84, 80]\n",
            "Layer 1: Active tokens per batch = [80, 78, 75, 81, 76, 78, 79, 78, 69, 79, 76, 69, 68, 80, 84, 80]\n",
            "Layer 0: Active tokens per batch = [70, 82, 76, 77, 80, 67, 68, 78, 78, 82, 72, 79, 75, 77, 81, 86]\n",
            "Layer 1: Active tokens per batch = [70, 82, 76, 77, 80, 67, 68, 78, 78, 82, 72, 79, 75, 77, 81, 86]\n",
            "Layer 0: Active tokens per batch = [84, 74, 74, 71, 74, 81, 66, 74, 70, 56, 82, 77, 73, 83, 79, 73]\n",
            "Layer 1: Active tokens per batch = [84, 74, 74, 71, 74, 81, 66, 74, 70, 56, 82, 77, 73, 83, 79, 73]\n",
            "Layer 0: Active tokens per batch = [69, 70, 77, 82, 82, 77, 74, 78, 69, 91, 73, 78, 68, 72, 72, 68]\n",
            "Layer 1: Active tokens per batch = [69, 70, 77, 82, 82, 77, 74, 78, 69, 91, 73, 78, 68, 72, 72, 68]\n",
            "Layer 0: Active tokens per batch = [72, 75, 67, 76, 86, 77, 83, 73, 73, 78, 71, 80, 70, 75, 77, 76]\n",
            "Layer 1: Active tokens per batch = [72, 75, 67, 76, 86, 77, 83, 73, 73, 78, 71, 80, 70, 75, 77, 76]\n",
            "Layer 0: Active tokens per batch = [79, 80, 71, 77, 73, 69, 71, 77, 78, 65, 75, 79, 79, 77, 79, 75]\n",
            "Layer 1: Active tokens per batch = [79, 80, 71, 77, 73, 69, 71, 77, 78, 65, 75, 79, 79, 77, 79, 75]\n",
            "Layer 0: Active tokens per batch = [78, 77, 70, 74, 86, 78, 80, 76, 82, 67, 67, 76, 64, 78, 74, 81]\n",
            "Layer 1: Active tokens per batch = [78, 77, 70, 74, 86, 78, 80, 76, 82, 67, 67, 76, 64, 78, 74, 81]\n",
            "Layer 0: Active tokens per batch = [73, 75, 77, 72, 74, 78, 80, 69, 74, 86, 79, 75, 85, 84, 84, 81]\n",
            "Layer 1: Active tokens per batch = [73, 75, 77, 72, 74, 78, 80, 69, 74, 86, 79, 75, 85, 84, 84, 81]\n",
            "Layer 0: Active tokens per batch = [74, 76, 80, 78, 72, 74, 76, 67, 70, 74, 72, 78, 80, 75, 77, 80]\n",
            "Layer 1: Active tokens per batch = [74, 76, 80, 78, 72, 74, 76, 67, 70, 74, 72, 78, 80, 75, 77, 80]\n",
            "Layer 0: Active tokens per batch = [77, 82, 85, 78, 77, 70, 67, 80, 72, 72, 75, 94, 62, 87, 78, 77]\n",
            "Layer 1: Active tokens per batch = [77, 82, 85, 78, 77, 70, 67, 80, 72, 72, 75, 94, 62, 87, 78, 77]\n",
            "Layer 0: Active tokens per batch = [82, 72, 71, 84, 76, 80, 67, 76, 68, 75, 72, 86, 78, 65, 77, 76]\n",
            "Layer 1: Active tokens per batch = [82, 72, 71, 84, 76, 80, 67, 76, 68, 75, 72, 86, 78, 65, 77, 76]\n",
            "Layer 0: Active tokens per batch = [71, 62, 68, 75, 78, 78, 79, 73, 74, 66, 64, 71, 71, 80, 72, 80]\n",
            "Layer 1: Active tokens per batch = [71, 62, 68, 75, 78, 78, 79, 73, 74, 66, 64, 71, 71, 80, 72, 80]\n",
            "Layer 0: Active tokens per batch = [67, 73, 61, 75, 85, 77, 79, 88, 78, 72, 84, 75, 72, 80, 72, 74]\n",
            "Layer 1: Active tokens per batch = [67, 73, 61, 75, 85, 77, 79, 88, 78, 72, 84, 75, 72, 80, 72, 74]\n",
            "Layer 0: Active tokens per batch = [81, 76, 75, 73, 86, 69, 81, 76, 82, 75, 68, 81, 73, 85, 80, 79]\n",
            "Layer 1: Active tokens per batch = [81, 76, 75, 73, 86, 69, 81, 76, 82, 75, 68, 81, 73, 85, 80, 79]\n",
            "Layer 0: Active tokens per batch = [75, 76, 76, 71, 74, 77, 82, 77, 78, 88, 77, 70, 69, 71, 78, 75]\n",
            "Layer 1: Active tokens per batch = [75, 76, 76, 71, 74, 77, 82, 77, 78, 88, 77, 70, 69, 71, 78, 75]\n",
            "Layer 0: Active tokens per batch = [78, 76, 72, 74, 79, 69, 73, 80, 91, 84, 78, 76, 84, 77, 78, 83]\n",
            "Layer 1: Active tokens per batch = [78, 76, 72, 74, 79, 69, 73, 80, 91, 84, 78, 76, 84, 77, 78, 83]\n",
            "Layer 0: Active tokens per batch = [78, 82, 79, 72, 78, 74, 71, 82, 79, 85, 72, 73, 74, 77, 80, 73]\n",
            "Layer 1: Active tokens per batch = [78, 82, 79, 72, 78, 74, 71, 82, 79, 85, 72, 73, 74, 77, 80, 73]\n",
            "Layer 0: Active tokens per batch = [81, 66, 75, 81, 74, 78, 74, 76, 71, 69, 72, 83, 68, 74, 71, 82]\n",
            "Layer 1: Active tokens per batch = [81, 66, 75, 81, 74, 78, 74, 76, 71, 69, 72, 83, 68, 74, 71, 82]\n",
            "Layer 0: Active tokens per batch = [75, 77, 84, 83, 71, 67, 72, 67, 77, 79, 69, 77, 72, 75, 78, 84]\n",
            "Layer 1: Active tokens per batch = [75, 77, 84, 83, 71, 67, 72, 67, 77, 79, 69, 77, 72, 75, 78, 84]\n",
            "Layer 0: Active tokens per batch = [80, 84, 85, 86, 78, 80, 77, 66, 75, 80, 81, 84, 69, 82, 84, 77]\n",
            "Layer 1: Active tokens per batch = [80, 84, 85, 86, 78, 80, 77, 66, 75, 80, 81, 84, 69, 82, 84, 77]\n",
            "Layer 0: Active tokens per batch = [75, 72, 71, 82, 64, 77, 71, 79, 74, 83, 63, 84, 75, 63, 74, 81]\n",
            "Layer 1: Active tokens per batch = [75, 72, 71, 82, 64, 77, 71, 79, 74, 83, 63, 84, 75, 63, 74, 81]\n",
            "Layer 0: Active tokens per batch = [78, 80, 75, 68, 74, 78, 74, 86, 64, 76, 76, 83, 83, 74, 76, 67]\n",
            "Layer 1: Active tokens per batch = [78, 80, 75, 68, 74, 78, 74, 86, 64, 76, 76, 83, 83, 74, 76, 67]\n",
            "Layer 0: Active tokens per batch = [79, 82, 82, 84, 80, 81, 96, 81, 77, 84, 67, 71, 73, 72, 83, 78]\n",
            "Layer 1: Active tokens per batch = [79, 82, 82, 84, 80, 81, 96, 81, 77, 84, 67, 71, 73, 72, 83, 78]\n",
            "Layer 0: Active tokens per batch = [74, 82, 70, 69, 76, 81, 79, 72, 79, 78, 76, 84, 69, 78, 76, 71]\n",
            "Layer 1: Active tokens per batch = [74, 82, 70, 69, 76, 81, 79, 72, 79, 78, 76, 84, 69, 78, 76, 71]\n",
            "Layer 0: Active tokens per batch = [77, 80, 88, 71, 79, 78, 76, 74, 75, 75, 72, 70, 75, 85, 82, 82]\n",
            "Layer 1: Active tokens per batch = [77, 80, 88, 71, 79, 78, 76, 74, 75, 75, 72, 70, 75, 85, 82, 82]\n",
            "Layer 0: Active tokens per batch = [83, 82, 85, 75, 77, 73, 80, 72, 79, 79, 80, 70, 81, 70, 79, 76]\n",
            "Layer 1: Active tokens per batch = [83, 82, 85, 75, 77, 73, 80, 72, 79, 79, 80, 70, 81, 70, 79, 76]\n",
            "Layer 0: Active tokens per batch = [75, 73, 79, 72, 73, 77, 86, 74, 70, 78, 82, 86, 74, 79, 74, 67]\n",
            "Layer 1: Active tokens per batch = [75, 73, 79, 72, 73, 77, 86, 74, 70, 78, 82, 86, 74, 79, 74, 67]\n",
            "Layer 0: Active tokens per batch = [82, 75, 76, 72, 82, 65, 76, 72, 73, 80, 71, 80, 76, 65, 76, 76]\n",
            "Layer 1: Active tokens per batch = [82, 75, 76, 72, 82, 65, 76, 72, 73, 80, 71, 80, 76, 65, 76, 76]\n",
            "Layer 0: Active tokens per batch = [66, 69, 75, 73, 71, 87, 80, 76, 69, 71, 78, 86, 68, 75, 80, 74]\n",
            "Layer 1: Active tokens per batch = [66, 69, 75, 73, 71, 87, 80, 76, 69, 71, 78, 86, 68, 75, 80, 74]\n",
            "Layer 0: Active tokens per batch = [75, 70, 70, 71, 79, 69, 72, 79, 73, 87, 78, 77, 79, 65, 84, 85]\n",
            "Layer 1: Active tokens per batch = [75, 70, 70, 71, 79, 69, 72, 79, 73, 87, 78, 77, 79, 65, 84, 85]\n",
            "Layer 0: Active tokens per batch = [83, 76, 69, 76, 79, 82, 80, 74, 69, 82, 82, 79, 64, 72, 80, 86]\n",
            "Layer 1: Active tokens per batch = [83, 76, 69, 76, 79, 82, 80, 74, 69, 82, 82, 79, 64, 72, 80, 86]\n",
            "Layer 0: Active tokens per batch = [78, 76, 69, 81, 68, 77, 80, 77, 73, 73, 78, 79, 75, 70, 71, 72]\n",
            "Layer 1: Active tokens per batch = [78, 76, 69, 81, 68, 77, 80, 77, 73, 73, 78, 79, 75, 70, 71, 72]\n",
            "Layer 0: Active tokens per batch = [78, 79, 82, 73, 90, 84, 79, 71, 79, 74, 74, 74, 70, 84, 74, 79]\n",
            "Layer 1: Active tokens per batch = [78, 79, 82, 73, 90, 84, 79, 71, 79, 74, 74, 74, 70, 84, 74, 79]\n",
            "Layer 0: Active tokens per batch = [79, 76, 80, 75, 79, 67, 67, 76, 86, 80, 86, 74, 78, 80, 76, 76]\n",
            "Layer 1: Active tokens per batch = [79, 76, 80, 75, 79, 67, 67, 76, 86, 80, 86, 74, 78, 80, 76, 76]\n",
            "Layer 0: Active tokens per batch = [75, 78, 87, 77, 70, 76, 80, 79, 73, 68, 85, 83, 82, 74, 74, 71]\n",
            "Layer 1: Active tokens per batch = [75, 78, 87, 77, 70, 76, 80, 79, 73, 68, 85, 83, 82, 74, 74, 71]\n",
            "Layer 0: Active tokens per batch = [87, 79, 70, 84, 77, 76, 78, 72, 82, 85, 74, 76, 80, 71, 68, 79]\n",
            "Layer 1: Active tokens per batch = [87, 79, 70, 84, 77, 76, 78, 72, 82, 85, 74, 76, 80, 71, 68, 79]\n",
            "Layer 0: Active tokens per batch = [71, 78, 65, 80, 76, 73, 70, 71, 85, 83, 79, 77, 78, 79, 74, 75]\n",
            "Layer 1: Active tokens per batch = [71, 78, 65, 80, 76, 73, 70, 71, 85, 83, 79, 77, 78, 79, 74, 75]\n",
            "Layer 0: Active tokens per batch = [76, 81, 73, 85, 73, 89, 73, 78, 67, 85, 76, 76, 67, 81, 72, 70]\n",
            "Layer 1: Active tokens per batch = [76, 81, 73, 85, 73, 89, 73, 78, 67, 85, 76, 76, 67, 81, 72, 70]\n",
            "Layer 0: Active tokens per batch = [79, 75, 70, 64, 68, 66, 77, 76, 79, 76, 83, 86, 74, 74, 79, 74]\n",
            "Layer 1: Active tokens per batch = [79, 75, 70, 64, 68, 66, 77, 76, 79, 76, 83, 86, 74, 74, 79, 74]\n",
            "Layer 0: Active tokens per batch = [71, 72, 70, 79, 75, 76, 77, 71, 68, 85, 77, 84, 75, 67, 77, 85]\n",
            "Layer 1: Active tokens per batch = [71, 72, 70, 79, 75, 76, 77, 71, 68, 85, 77, 84, 75, 67, 77, 85]\n",
            "Layer 0: Active tokens per batch = [75, 78, 79, 67, 75, 80, 77, 77, 77, 64, 69, 76, 77, 77, 76, 83]\n",
            "Layer 1: Active tokens per batch = [75, 78, 79, 67, 75, 80, 77, 77, 77, 64, 69, 76, 77, 77, 76, 83]\n",
            "Layer 0: Active tokens per batch = [70, 81, 75, 77, 78, 72, 79, 72, 77, 70, 78, 78, 84, 77, 75, 74]\n",
            "Layer 1: Active tokens per batch = [70, 81, 75, 77, 78, 72, 79, 72, 77, 70, 78, 78, 84, 77, 75, 74]\n",
            "Layer 0: Active tokens per batch = [73, 77, 75, 73, 75, 77, 71, 79, 73, 79, 71, 86, 69, 77, 78, 77]\n",
            "Layer 1: Active tokens per batch = [73, 77, 75, 73, 75, 77, 71, 79, 73, 79, 71, 86, 69, 77, 78, 77]\n",
            "Layer 0: Active tokens per batch = [71, 69, 67, 66, 80, 80, 73, 74, 75, 82, 70, 70, 78, 74, 69, 68]\n",
            "Layer 1: Active tokens per batch = [71, 69, 67, 66, 80, 80, 73, 74, 75, 82, 70, 70, 78, 74, 69, 68]\n",
            "Layer 0: Active tokens per batch = [87, 77, 80, 83, 82, 74, 79, 74, 80, 78, 77, 79, 75, 70, 72, 79]\n",
            "Layer 1: Active tokens per batch = [87, 77, 80, 83, 82, 74, 79, 74, 80, 78, 77, 79, 75, 70, 72, 79]\n",
            "Layer 0: Active tokens per batch = [67, 79, 83, 78, 73, 76, 82, 66, 78, 64, 81, 67, 85, 70, 72, 75]\n",
            "Layer 1: Active tokens per batch = [67, 79, 83, 78, 73, 76, 82, 66, 78, 64, 81, 67, 85, 70, 72, 75]\n",
            "Layer 0: Active tokens per batch = [63, 81, 75, 68, 81, 66, 77, 78, 82, 73, 87, 80, 75, 76, 74, 77]\n",
            "Layer 1: Active tokens per batch = [63, 81, 75, 68, 81, 66, 77, 78, 82, 73, 87, 80, 75, 76, 74, 77]\n",
            "Layer 0: Active tokens per batch = [80, 82, 79, 81, 73, 73, 70, 78, 79, 77, 71, 87, 77, 82, 88, 77]\n",
            "Layer 1: Active tokens per batch = [80, 82, 79, 81, 73, 73, 70, 78, 79, 77, 71, 87, 77, 82, 88, 77]\n",
            "Layer 0: Active tokens per batch = [86, 73, 67, 70, 75, 74, 81, 75, 78, 69, 71, 66, 76, 78, 73, 69]\n",
            "Layer 1: Active tokens per batch = [86, 73, 67, 70, 75, 74, 81, 75, 78, 69, 71, 66, 76, 78, 73, 69]\n",
            "Layer 0: Active tokens per batch = [80, 76, 77, 79, 80, 75, 79, 75, 72, 77, 82, 78, 74, 85, 74, 78]\n",
            "Layer 1: Active tokens per batch = [80, 76, 77, 79, 80, 75, 79, 75, 72, 77, 82, 78, 74, 85, 74, 78]\n",
            "Layer 0: Active tokens per batch = [76, 69, 76, 80, 74, 82, 76, 85, 80, 74, 81, 84, 69, 78, 78, 72]\n",
            "Layer 1: Active tokens per batch = [76, 69, 76, 80, 74, 82, 76, 85, 80, 74, 81, 84, 69, 78, 78, 72]\n",
            "Layer 0: Active tokens per batch = [83, 74, 74, 73, 73, 80, 78, 74, 82, 71, 67, 72, 77, 83, 82, 71]\n",
            "Layer 1: Active tokens per batch = [83, 74, 74, 73, 73, 80, 78, 74, 82, 71, 67, 72, 77, 83, 82, 71]\n",
            "Layer 0: Active tokens per batch = [75, 71, 73, 85, 83, 68, 82, 71]\n",
            "Layer 1: Active tokens per batch = [75, 71, 73, 85, 83, 68, 82, 71]\n",
            "Layer 0: Active tokens per batch = [74, 86, 85, 78, 71, 81, 76, 82, 85, 69, 75, 72, 88, 80, 74, 75]\n",
            "Layer 1: Active tokens per batch = [74, 86, 85, 78, 71, 81, 76, 82, 85, 69, 75, 72, 88, 80, 74, 75]\n",
            "Layer 0: Active tokens per batch = [78, 75, 68, 75, 71, 74, 74, 70, 78, 73, 79, 79, 78, 77, 83, 72]\n",
            "Layer 1: Active tokens per batch = [78, 75, 68, 75, 71, 74, 74, 70, 78, 73, 79, 79, 78, 77, 83, 72]\n",
            "Layer 0: Active tokens per batch = [77, 76, 69, 78, 70, 81, 80, 70, 86, 72, 69, 73, 80, 68, 75, 74]\n",
            "Layer 1: Active tokens per batch = [77, 76, 69, 78, 70, 81, 80, 70, 86, 72, 69, 73, 80, 68, 75, 74]\n",
            "Layer 0: Active tokens per batch = [77, 77, 78, 82, 77, 74, 69, 76, 87, 74, 80, 69, 67, 63, 66, 80]\n",
            "Layer 1: Active tokens per batch = [77, 77, 78, 82, 77, 74, 69, 76, 87, 74, 80, 69, 67, 63, 66, 80]\n",
            "Layer 0: Active tokens per batch = [76, 78, 77, 77, 78, 74, 87, 78, 69, 73, 77, 76, 79, 71, 81, 80]\n",
            "Layer 1: Active tokens per batch = [76, 78, 77, 77, 78, 74, 87, 78, 69, 73, 77, 76, 79, 71, 81, 80]\n",
            "Layer 0: Active tokens per batch = [73, 70, 74, 71, 75, 68, 76, 76, 81, 64, 77, 75, 75, 74, 82, 67]\n",
            "Layer 1: Active tokens per batch = [73, 70, 74, 71, 75, 68, 76, 76, 81, 64, 77, 75, 75, 74, 82, 67]\n",
            "Layer 0: Active tokens per batch = [75, 78, 71, 79, 78, 76, 70, 75, 74, 80, 86, 75, 78, 75, 81, 78]\n",
            "Layer 1: Active tokens per batch = [75, 78, 71, 79, 78, 76, 70, 75, 74, 80, 86, 75, 78, 75, 81, 78]\n",
            "Layer 0: Active tokens per batch = [79, 87, 82, 82, 82, 81, 82, 68, 78, 74, 65, 81, 84, 87, 77, 80]\n",
            "Layer 1: Active tokens per batch = [79, 87, 82, 82, 82, 81, 82, 68, 78, 74, 65, 81, 84, 87, 77, 80]\n",
            "Layer 0: Active tokens per batch = [75, 74, 71, 76, 86, 83, 69, 75, 75, 79, 68, 85, 71, 81, 77, 79]\n",
            "Layer 1: Active tokens per batch = [75, 74, 71, 76, 86, 83, 69, 75, 75, 79, 68, 85, 71, 81, 77, 79]\n",
            "Layer 0: Active tokens per batch = [69, 66, 79, 74, 84, 71, 76, 86, 73, 84, 75, 71, 83, 81, 82, 82]\n",
            "Layer 1: Active tokens per batch = [69, 66, 79, 74, 84, 71, 76, 86, 73, 84, 75, 71, 83, 81, 82, 82]\n",
            "Layer 0: Active tokens per batch = [71, 86, 70, 81, 74, 69, 78, 69, 74, 66, 80, 74, 80, 77, 68, 70]\n",
            "Layer 1: Active tokens per batch = [71, 86, 70, 81, 74, 69, 78, 69, 74, 66, 80, 74, 80, 77, 68, 70]\n",
            "Layer 0: Active tokens per batch = [80, 72, 85, 74, 69, 81, 78, 85, 70, 64, 79, 67, 75, 78, 69, 83]\n",
            "Layer 1: Active tokens per batch = [80, 72, 85, 74, 69, 81, 78, 85, 70, 64, 79, 67, 75, 78, 69, 83]\n",
            "Layer 0: Active tokens per batch = [79, 80, 84, 74, 81, 67, 70, 73, 87, 68, 80, 78, 85, 67, 79, 64]\n",
            "Layer 1: Active tokens per batch = [79, 80, 84, 74, 81, 67, 70, 73, 87, 68, 80, 78, 85, 67, 79, 64]\n",
            "Layer 0: Active tokens per batch = [77, 80, 79, 63, 79, 77, 77, 68, 69, 77, 76, 70, 79, 72, 71, 72]\n",
            "Layer 1: Active tokens per batch = [77, 80, 79, 63, 79, 77, 77, 68, 69, 77, 76, 70, 79, 72, 71, 72]\n",
            "Layer 0: Active tokens per batch = [83, 80, 75, 85, 76, 75, 84, 78, 81, 73, 79, 72, 75, 74, 73, 77]\n",
            "Layer 1: Active tokens per batch = [83, 80, 75, 85, 76, 75, 84, 78, 81, 73, 79, 72, 75, 74, 73, 77]\n",
            "Layer 0: Active tokens per batch = [84, 74, 76, 71, 79, 81, 79, 73, 81, 82, 73, 71, 91, 75, 73, 68]\n",
            "Layer 1: Active tokens per batch = [84, 74, 76, 71, 79, 81, 79, 73, 81, 82, 73, 71, 91, 75, 73, 68]\n",
            "Layer 0: Active tokens per batch = [80, 83, 74, 82, 69, 70, 78, 73, 77, 71, 76, 75, 68, 75, 80, 79]\n",
            "Layer 1: Active tokens per batch = [80, 83, 74, 82, 69, 70, 78, 73, 77, 71, 76, 75, 68, 75, 80, 79]\n",
            "Layer 0: Active tokens per batch = [73, 76, 80, 84, 82, 89, 84, 84, 81, 72, 77, 76, 74, 72, 66, 65]\n",
            "Layer 1: Active tokens per batch = [73, 76, 80, 84, 82, 89, 84, 84, 81, 72, 77, 76, 74, 72, 66, 65]\n",
            "Layer 0: Active tokens per batch = [68, 71, 83, 62, 77, 74, 84, 67, 86, 75, 81, 80, 72, 74, 75, 66]\n",
            "Layer 1: Active tokens per batch = [68, 71, 83, 62, 77, 74, 84, 67, 86, 75, 81, 80, 72, 74, 75, 66]\n",
            "Layer 0: Active tokens per batch = [82, 80, 71, 86, 71, 85, 67, 78, 78, 82, 77, 67, 88, 77, 75, 77]\n",
            "Layer 1: Active tokens per batch = [82, 80, 71, 86, 71, 85, 67, 78, 78, 82, 77, 67, 88, 77, 75, 77]\n",
            "Layer 0: Active tokens per batch = [78, 77, 69, 73, 77, 89, 81, 85, 82, 69, 67, 73, 71, 80, 79, 77]\n",
            "Layer 1: Active tokens per batch = [78, 77, 69, 73, 77, 89, 81, 85, 82, 69, 67, 73, 71, 80, 79, 77]\n",
            "Layer 0: Active tokens per batch = [67, 68, 78, 86, 81, 79, 82, 73, 72, 84, 66, 81, 83, 78, 77, 72]\n",
            "Layer 1: Active tokens per batch = [67, 68, 78, 86, 81, 79, 82, 73, 72, 84, 66, 81, 83, 78, 77, 72]\n",
            "Layer 0: Active tokens per batch = [70, 71, 71, 86, 78, 76, 72, 88, 84, 77, 75, 78, 82, 75, 78, 77]\n",
            "Layer 1: Active tokens per batch = [70, 71, 71, 86, 78, 76, 72, 88, 84, 77, 75, 78, 82, 75, 78, 77]\n",
            "Layer 0: Active tokens per batch = [77, 78, 76, 87, 81, 77, 79, 72, 74, 76, 72, 77, 69, 74, 76, 71]\n",
            "Layer 1: Active tokens per batch = [77, 78, 76, 87, 81, 77, 79, 72, 74, 76, 72, 77, 69, 74, 76, 71]\n",
            "Layer 0: Active tokens per batch = [73, 85, 79, 77, 82, 81, 77, 71, 79, 78, 73, 86, 74, 71, 81, 81]\n",
            "Layer 1: Active tokens per batch = [73, 85, 79, 77, 82, 81, 77, 71, 79, 78, 73, 86, 74, 71, 81, 81]\n",
            "Layer 0: Active tokens per batch = [81, 79, 78, 72, 77, 69, 71, 75, 79, 80, 80, 75, 70, 80, 76, 90]\n",
            "Layer 1: Active tokens per batch = [81, 79, 78, 72, 77, 69, 71, 75, 79, 80, 80, 75, 70, 80, 76, 90]\n",
            "Layer 0: Active tokens per batch = [84, 73, 78, 72, 68, 79, 76, 79, 67, 73, 82, 80, 73, 75, 79, 67]\n",
            "Layer 1: Active tokens per batch = [84, 73, 78, 72, 68, 79, 76, 79, 67, 73, 82, 80, 73, 75, 79, 67]\n",
            "Layer 0: Active tokens per batch = [74, 75, 75, 78, 72, 87, 73, 74, 80, 82, 72, 72, 74, 71, 80, 77]\n",
            "Layer 1: Active tokens per batch = [74, 75, 75, 78, 72, 87, 73, 74, 80, 82, 72, 72, 74, 71, 80, 77]\n",
            "Layer 0: Active tokens per batch = [71, 74, 72, 75, 76, 79, 79, 75, 73, 72, 84, 74, 65, 87, 74, 76]\n",
            "Layer 1: Active tokens per batch = [71, 74, 72, 75, 76, 79, 79, 75, 73, 72, 84, 74, 65, 87, 74, 76]\n",
            "Layer 0: Active tokens per batch = [73, 78, 82, 75, 77, 70, 80, 74, 76, 76, 79, 72, 81, 75, 84, 75]\n",
            "Layer 1: Active tokens per batch = [73, 78, 82, 75, 77, 70, 80, 74, 76, 76, 79, 72, 81, 75, 84, 75]\n",
            "Layer 0: Active tokens per batch = [80, 72, 74, 74, 76, 75, 74, 79, 75, 68, 70, 70, 71, 70, 82, 82]\n",
            "Layer 1: Active tokens per batch = [80, 72, 74, 74, 76, 75, 74, 79, 75, 68, 70, 70, 71, 70, 82, 82]\n",
            "Layer 0: Active tokens per batch = [76, 73, 86, 77, 76, 72, 82, 83, 74, 78, 75, 78, 70, 72, 75, 73]\n",
            "Layer 1: Active tokens per batch = [76, 73, 86, 77, 76, 72, 82, 83, 74, 78, 75, 78, 70, 72, 75, 73]\n",
            "Layer 0: Active tokens per batch = [72, 81, 77, 67, 85, 83, 86, 75, 83, 84, 74, 69, 78, 77, 86, 78]\n",
            "Layer 1: Active tokens per batch = [72, 81, 77, 67, 85, 83, 86, 75, 83, 84, 74, 69, 78, 77, 86, 78]\n",
            "Layer 0: Active tokens per batch = [79, 82, 75, 74, 78, 73, 86, 71, 67, 74, 78, 81, 79, 80, 78, 82]\n",
            "Layer 1: Active tokens per batch = [79, 82, 75, 74, 78, 73, 86, 71, 67, 74, 78, 81, 79, 80, 78, 82]\n",
            "Layer 0: Active tokens per batch = [66, 80, 76, 82, 78, 64, 81, 76, 69, 75, 72, 78, 65, 72, 74, 78]\n",
            "Layer 1: Active tokens per batch = [66, 80, 76, 82, 78, 64, 81, 76, 69, 75, 72, 78, 65, 72, 74, 78]\n",
            "Layer 0: Active tokens per batch = [72, 84, 82, 73, 80, 74, 77, 76, 83, 78, 72, 80, 91, 68, 81, 61]\n",
            "Layer 1: Active tokens per batch = [72, 84, 82, 73, 80, 74, 77, 76, 83, 78, 72, 80, 91, 68, 81, 61]\n",
            "Layer 0: Active tokens per batch = [67, 82, 82, 79, 72, 78, 75, 79, 77, 73, 67, 73, 77, 68, 73, 80]\n",
            "Layer 1: Active tokens per batch = [67, 82, 82, 79, 72, 78, 75, 79, 77, 73, 67, 73, 77, 68, 73, 80]\n",
            "Layer 0: Active tokens per batch = [82, 73, 74, 76, 76, 72, 78, 79, 75, 87, 76, 79, 75, 71, 80, 84]\n",
            "Layer 1: Active tokens per batch = [82, 73, 74, 76, 76, 72, 78, 79, 75, 87, 76, 79, 75, 71, 80, 84]\n",
            "Layer 0: Active tokens per batch = [69, 74, 69, 83, 79, 63, 78, 74, 76, 73, 75, 77, 76, 80, 70, 71]\n",
            "Layer 1: Active tokens per batch = [69, 74, 69, 83, 79, 63, 78, 74, 76, 73, 75, 77, 76, 80, 70, 71]\n",
            "Layer 0: Active tokens per batch = [73, 82, 71, 76, 81, 79, 78, 70, 78, 78, 79, 64, 72, 74, 79, 73]\n",
            "Layer 1: Active tokens per batch = [73, 82, 71, 76, 81, 79, 78, 70, 78, 78, 79, 64, 72, 74, 79, 73]\n",
            "Layer 0: Active tokens per batch = [79, 72, 79, 79, 78, 56, 82, 80, 67, 64, 80, 73, 79, 70, 75, 71]\n",
            "Layer 1: Active tokens per batch = [79, 72, 79, 79, 78, 56, 82, 80, 67, 64, 80, 73, 79, 70, 75, 71]\n",
            "Layer 0: Active tokens per batch = [69, 77, 70, 67, 74, 74, 77, 81, 80, 68, 82, 91, 83, 78, 81, 70]\n",
            "Layer 1: Active tokens per batch = [69, 77, 70, 67, 74, 74, 77, 81, 80, 68, 82, 91, 83, 78, 81, 70]\n",
            "Layer 0: Active tokens per batch = [74, 69, 85, 71, 76, 75, 71, 96, 79, 77, 72, 79, 81, 73, 71, 65]\n",
            "Layer 1: Active tokens per batch = [74, 69, 85, 71, 76, 75, 71, 96, 79, 77, 72, 79, 81, 73, 71, 65]\n",
            "Layer 0: Active tokens per batch = [76, 80, 70, 80, 78, 86, 78, 78, 76, 68, 74, 71, 75, 74, 80, 71]\n",
            "Layer 1: Active tokens per batch = [76, 80, 70, 80, 78, 86, 78, 78, 76, 68, 74, 71, 75, 74, 80, 71]\n",
            "Layer 0: Active tokens per batch = [76, 84, 73, 77, 79, 82, 77, 80, 80, 79, 79, 73, 88, 80, 72, 79]\n",
            "Layer 1: Active tokens per batch = [76, 84, 73, 77, 79, 82, 77, 80, 80, 79, 79, 73, 88, 80, 72, 79]\n",
            "Layer 0: Active tokens per batch = [75, 70, 85, 78, 73, 70, 76, 77, 82, 78, 69, 72, 79, 73, 80, 79]\n",
            "Layer 1: Active tokens per batch = [75, 70, 85, 78, 73, 70, 76, 77, 82, 78, 69, 72, 79, 73, 80, 79]\n",
            "Layer 0: Active tokens per batch = [76, 85, 77, 84, 81, 67, 76, 74, 84, 70, 79, 82, 77, 82, 70, 74]\n",
            "Layer 1: Active tokens per batch = [76, 85, 77, 84, 81, 67, 76, 74, 84, 70, 79, 82, 77, 82, 70, 74]\n",
            "Layer 0: Active tokens per batch = [69, 66, 75, 69, 85, 75, 74, 85, 79, 76, 70, 62, 76, 64, 84, 71]\n",
            "Layer 1: Active tokens per batch = [69, 66, 75, 69, 85, 75, 74, 85, 79, 76, 70, 62, 76, 64, 84, 71]\n",
            "Layer 0: Active tokens per batch = [76, 76, 73, 79, 75, 83, 77, 85, 86, 78, 82, 69, 81, 70, 70, 81]\n",
            "Layer 1: Active tokens per batch = [76, 76, 73, 79, 75, 83, 77, 85, 86, 78, 82, 69, 81, 70, 70, 81]\n",
            "Layer 0: Active tokens per batch = [79, 94, 77, 65, 80, 74, 81, 78, 75, 75, 77, 82, 71, 76, 80, 79]\n",
            "Layer 1: Active tokens per batch = [79, 94, 77, 65, 80, 74, 81, 78, 75, 75, 77, 82, 71, 76, 80, 79]\n",
            "Layer 0: Active tokens per batch = [76, 72, 69, 76, 70, 80, 74, 77, 76, 66, 70, 86, 79, 72, 80, 74]\n",
            "Layer 1: Active tokens per batch = [76, 72, 69, 76, 70, 80, 74, 77, 76, 66, 70, 86, 79, 72, 80, 74]\n",
            "Layer 0: Active tokens per batch = [77, 73, 78, 74, 66, 78, 80, 74, 72, 78, 76, 75, 78, 82, 85, 77]\n",
            "Layer 1: Active tokens per batch = [77, 73, 78, 74, 66, 78, 80, 74, 72, 78, 76, 75, 78, 82, 85, 77]\n",
            "Layer 0: Active tokens per batch = [78, 75, 67, 85, 72, 71, 82, 74, 66, 77, 84, 75, 71, 78, 87, 76]\n",
            "Layer 1: Active tokens per batch = [78, 75, 67, 85, 72, 71, 82, 74, 66, 77, 84, 75, 71, 78, 87, 76]\n",
            "Layer 0: Active tokens per batch = [72, 74, 75, 80, 77, 77, 68, 80, 73, 85, 77, 80, 69, 80, 73, 76]\n",
            "Layer 1: Active tokens per batch = [72, 74, 75, 80, 77, 77, 68, 80, 73, 85, 77, 80, 69, 80, 73, 76]\n",
            "Layer 0: Active tokens per batch = [69, 76, 69, 83, 76, 70, 74, 71, 83, 71, 76, 74, 77, 78, 79, 70]\n",
            "Layer 1: Active tokens per batch = [69, 76, 69, 83, 76, 70, 74, 71, 83, 71, 76, 74, 77, 78, 79, 70]\n",
            "Layer 0: Active tokens per batch = [72, 79, 84, 85, 70, 70, 83, 77, 83, 81, 73, 80, 80, 76, 72, 70]\n",
            "Layer 1: Active tokens per batch = [72, 79, 84, 85, 70, 70, 83, 77, 83, 81, 73, 80, 80, 76, 72, 70]\n",
            "Layer 0: Active tokens per batch = [83, 69, 70, 73, 77, 78, 77, 76, 77, 79, 64, 65, 64, 82, 74, 68]\n",
            "Layer 1: Active tokens per batch = [83, 69, 70, 73, 77, 78, 77, 76, 77, 79, 64, 65, 64, 82, 74, 68]\n",
            "Layer 0: Active tokens per batch = [70, 75, 64, 79, 84, 71, 78, 77, 71, 76, 82, 85, 76, 77, 80, 71]\n",
            "Layer 1: Active tokens per batch = [70, 75, 64, 79, 84, 71, 78, 77, 71, 76, 82, 85, 76, 77, 80, 71]\n",
            "Layer 0: Active tokens per batch = [80, 80, 81, 79, 86, 77, 79, 82, 73, 77, 83, 82, 84, 82, 71, 73]\n",
            "Layer 1: Active tokens per batch = [80, 80, 81, 79, 86, 77, 79, 82, 73, 77, 83, 82, 84, 82, 71, 73]\n",
            "Layer 0: Active tokens per batch = [68, 72, 78, 75, 84, 74, 67, 66, 67, 75, 78, 80, 85, 76, 73, 83]\n",
            "Layer 1: Active tokens per batch = [68, 72, 78, 75, 84, 74, 67, 66, 67, 75, 78, 80, 85, 76, 73, 83]\n",
            "Layer 0: Active tokens per batch = [73, 67, 68, 75, 72, 78, 82, 68, 71, 86, 78, 71, 84, 81, 75, 69]\n",
            "Layer 1: Active tokens per batch = [73, 67, 68, 75, 72, 78, 82, 68, 71, 86, 78, 71, 84, 81, 75, 69]\n",
            "Layer 0: Active tokens per batch = [79, 80, 78, 68, 81, 74, 79, 67, 78, 76, 83, 74, 76, 80, 72, 74]\n",
            "Layer 1: Active tokens per batch = [79, 80, 78, 68, 81, 74, 79, 67, 78, 76, 83, 74, 76, 80, 72, 74]\n",
            "Layer 0: Active tokens per batch = [79, 73, 75, 77, 71, 71, 82, 78]\n",
            "Layer 1: Active tokens per batch = [79, 73, 75, 77, 71, 71, 82, 78]\n",
            "Layer 0: Active tokens per batch = [83, 87, 74, 76, 82, 86, 86, 75, 77, 69, 83, 86, 86, 78, 85, 84]\n",
            "Layer 1: Active tokens per batch = [83, 87, 74, 76, 82, 86, 86, 75, 77, 69, 83, 86, 86, 78, 85, 84]\n",
            "Layer 0: Active tokens per batch = [77, 75, 81, 78, 84, 72, 73, 84, 76, 70, 77, 75, 78, 74, 73, 85]\n",
            "Layer 1: Active tokens per batch = [77, 75, 81, 78, 84, 72, 73, 84, 76, 70, 77, 75, 78, 74, 73, 85]\n",
            "Layer 0: Active tokens per batch = [78, 69, 62, 68, 82, 71, 82, 77, 71, 66, 83, 77, 78, 76, 77, 77]\n",
            "Layer 1: Active tokens per batch = [78, 69, 62, 68, 82, 71, 82, 77, 71, 66, 83, 77, 78, 76, 77, 77]\n",
            "Layer 0: Active tokens per batch = [73, 69, 67, 79, 72, 72, 86, 86, 80, 72, 74, 76, 68, 78, 75, 72]\n",
            "Layer 1: Active tokens per batch = [73, 69, 67, 79, 72, 72, 86, 86, 80, 72, 74, 76, 68, 78, 75, 72]\n",
            "Layer 0: Active tokens per batch = [73, 69, 75, 82, 77, 80, 63, 74, 77, 81, 77, 70, 81, 77, 63, 80]\n",
            "Layer 1: Active tokens per batch = [73, 69, 75, 82, 77, 80, 63, 74, 77, 81, 77, 70, 81, 77, 63, 80]\n",
            "Layer 0: Active tokens per batch = [80, 75, 79, 75, 71, 74, 79, 78, 71, 83, 71, 70, 72, 72, 75, 75]\n",
            "Layer 1: Active tokens per batch = [80, 75, 79, 75, 71, 74, 79, 78, 71, 83, 71, 70, 72, 72, 75, 75]\n",
            "Layer 0: Active tokens per batch = [73, 71, 75, 80, 78, 78, 80, 82, 67, 78, 81, 66, 77, 82, 75, 67]\n",
            "Layer 1: Active tokens per batch = [73, 71, 75, 80, 78, 78, 80, 82, 67, 78, 81, 66, 77, 82, 75, 67]\n",
            "Layer 0: Active tokens per batch = [85, 68, 71, 84, 78, 74, 73, 80, 85, 68, 76, 72, 83, 78, 74, 80]\n",
            "Layer 1: Active tokens per batch = [85, 68, 71, 84, 78, 74, 73, 80, 85, 68, 76, 72, 83, 78, 74, 80]\n",
            "Layer 0: Active tokens per batch = [77, 75, 81, 72, 83, 84, 73, 86, 74, 69, 72, 71, 64, 81, 76, 65]\n",
            "Layer 1: Active tokens per batch = [77, 75, 81, 72, 83, 84, 73, 86, 74, 69, 72, 71, 64, 81, 76, 65]\n",
            "Layer 0: Active tokens per batch = [80, 76, 75, 72, 76, 80, 85, 73, 75, 76, 71, 79, 71, 69, 65, 79]\n",
            "Layer 1: Active tokens per batch = [80, 76, 75, 72, 76, 80, 85, 73, 75, 76, 71, 79, 71, 69, 65, 79]\n",
            "Layer 0: Active tokens per batch = [80, 84, 78, 86, 81, 72, 78, 75, 80, 70, 70, 74, 71, 73, 85, 78]\n",
            "Layer 1: Active tokens per batch = [80, 84, 78, 86, 81, 72, 78, 75, 80, 70, 70, 74, 71, 73, 85, 78]\n",
            "Layer 0: Active tokens per batch = [71, 71, 80, 69, 79, 78, 78, 74, 78, 70, 68, 72, 71, 81, 74, 74]\n",
            "Layer 1: Active tokens per batch = [71, 71, 80, 69, 79, 78, 78, 74, 78, 70, 68, 72, 71, 81, 74, 74]\n",
            "Layer 0: Active tokens per batch = [75, 66, 79, 72, 67, 78, 70, 82, 86, 74, 85, 80, 73, 73, 80, 65]\n",
            "Layer 1: Active tokens per batch = [75, 66, 79, 72, 67, 78, 70, 82, 86, 74, 85, 80, 73, 73, 80, 65]\n",
            "Layer 0: Active tokens per batch = [79, 65, 67, 76, 69, 83, 74, 71, 80, 76, 64, 79, 76, 70, 80, 73]\n",
            "Layer 1: Active tokens per batch = [79, 65, 67, 76, 69, 83, 74, 71, 80, 76, 64, 79, 76, 70, 80, 73]\n",
            "Layer 0: Active tokens per batch = [81, 74, 84, 82, 82, 79, 70, 70, 75, 66, 68, 72, 74, 70, 64, 79]\n",
            "Layer 1: Active tokens per batch = [81, 74, 84, 82, 82, 79, 70, 70, 75, 66, 68, 72, 74, 70, 64, 79]\n",
            "Layer 0: Active tokens per batch = [82, 76, 84, 70, 68, 79, 85, 78, 78, 67, 77, 80, 88, 78, 71, 83]\n",
            "Layer 1: Active tokens per batch = [82, 76, 84, 70, 68, 79, 85, 78, 78, 67, 77, 80, 88, 78, 71, 83]\n",
            "Layer 0: Active tokens per batch = [75, 79, 70, 84, 82, 75, 80, 75, 69, 76, 74, 69, 78, 75, 87, 70]\n",
            "Layer 1: Active tokens per batch = [75, 79, 70, 84, 82, 75, 80, 75, 69, 76, 74, 69, 78, 75, 87, 70]\n",
            "Layer 0: Active tokens per batch = [77, 65, 76, 81, 72, 76, 75, 73, 74, 78, 77, 77, 80, 76, 74, 72]\n",
            "Layer 1: Active tokens per batch = [77, 65, 76, 81, 72, 76, 75, 73, 74, 78, 77, 77, 80, 76, 74, 72]\n",
            "Layer 0: Active tokens per batch = [79, 82, 83, 72, 82, 66, 82, 78, 73, 71, 77, 79, 83, 74, 87, 77]\n",
            "Layer 1: Active tokens per batch = [79, 82, 83, 72, 82, 66, 82, 78, 73, 71, 77, 79, 83, 74, 87, 77]\n",
            "Layer 0: Active tokens per batch = [80, 74, 74, 71, 78, 80, 73, 73, 66, 76, 76, 70, 91, 65, 74, 76]\n",
            "Layer 1: Active tokens per batch = [80, 74, 74, 71, 78, 80, 73, 73, 66, 76, 76, 70, 91, 65, 74, 76]\n",
            "Layer 0: Active tokens per batch = [73, 75, 85, 73, 78, 79, 80, 83, 74, 72, 81, 78, 71, 73, 82, 67]\n",
            "Layer 1: Active tokens per batch = [73, 75, 85, 73, 78, 79, 80, 83, 74, 72, 81, 78, 71, 73, 82, 67]\n",
            "Layer 0: Active tokens per batch = [77, 80, 71, 79, 84, 78, 64, 82, 75, 83, 84, 70, 67, 84, 73, 79]\n",
            "Layer 1: Active tokens per batch = [77, 80, 71, 79, 84, 78, 64, 82, 75, 83, 84, 70, 67, 84, 73, 79]\n",
            "Layer 0: Active tokens per batch = [73, 69, 78, 69, 80, 67, 74, 78, 78, 75, 81, 74, 81, 70, 75, 79]\n",
            "Layer 1: Active tokens per batch = [73, 69, 78, 69, 80, 67, 74, 78, 78, 75, 81, 74, 81, 70, 75, 79]\n",
            "Layer 0: Active tokens per batch = [69, 75, 79, 75, 76, 85, 82, 78, 75, 78, 81, 81, 74, 77, 76, 82]\n",
            "Layer 1: Active tokens per batch = [69, 75, 79, 75, 76, 85, 82, 78, 75, 78, 81, 81, 74, 77, 76, 82]\n",
            "Layer 0: Active tokens per batch = [87, 72, 73, 76, 78, 72, 81, 87, 73, 77, 78, 85, 74, 71, 78, 73]\n",
            "Layer 1: Active tokens per batch = [87, 72, 73, 76, 78, 72, 81, 87, 73, 77, 78, 85, 74, 71, 78, 73]\n",
            "Layer 0: Active tokens per batch = [80, 75, 74, 77, 73, 79, 80, 78, 87, 72, 84, 78, 73, 78, 79, 74]\n",
            "Layer 1: Active tokens per batch = [80, 75, 74, 77, 73, 79, 80, 78, 87, 72, 84, 78, 73, 78, 79, 74]\n",
            "Layer 0: Active tokens per batch = [74, 83, 75, 80, 70, 78, 71, 78, 87, 84, 83, 74, 74, 72, 72, 77]\n",
            "Layer 1: Active tokens per batch = [74, 83, 75, 80, 70, 78, 71, 78, 87, 84, 83, 74, 74, 72, 72, 77]\n",
            "Layer 0: Active tokens per batch = [73, 81, 80, 80, 85, 80, 80, 75, 73, 85, 82, 82, 74, 75, 73, 68]\n",
            "Layer 1: Active tokens per batch = [73, 81, 80, 80, 85, 80, 80, 75, 73, 85, 82, 82, 74, 75, 73, 68]\n",
            "Layer 0: Active tokens per batch = [75, 70, 80, 66, 70, 67, 84, 73, 85, 76, 79, 70, 79, 70, 68, 72]\n",
            "Layer 1: Active tokens per batch = [75, 70, 80, 66, 70, 67, 84, 73, 85, 76, 79, 70, 79, 70, 68, 72]\n",
            "Layer 0: Active tokens per batch = [76, 56, 75, 75, 82, 84, 84, 71, 76, 76, 84, 72, 75, 75, 79, 77]\n",
            "Layer 1: Active tokens per batch = [76, 56, 75, 75, 82, 84, 84, 71, 76, 76, 84, 72, 75, 75, 79, 77]\n",
            "Layer 0: Active tokens per batch = [79, 75, 70, 78, 81, 75, 76, 75, 78, 77, 70, 75, 79, 78, 77, 73]\n",
            "Layer 1: Active tokens per batch = [79, 75, 70, 78, 81, 75, 76, 75, 78, 77, 70, 75, 79, 78, 77, 73]\n",
            "Layer 0: Active tokens per batch = [71, 79, 75, 82, 75, 80, 71, 80, 79, 72, 67, 86, 85, 74, 78, 69]\n",
            "Layer 1: Active tokens per batch = [71, 79, 75, 82, 75, 80, 71, 80, 79, 72, 67, 86, 85, 74, 78, 69]\n",
            "Layer 0: Active tokens per batch = [74, 64, 86, 88, 78, 78, 76, 78, 74, 81, 81, 73, 77, 75, 76, 77]\n",
            "Layer 1: Active tokens per batch = [74, 64, 86, 88, 78, 78, 76, 78, 74, 81, 81, 73, 77, 75, 76, 77]\n",
            "Layer 0: Active tokens per batch = [83, 76, 79, 83, 76, 77, 67, 80, 67, 76, 74, 78, 78, 67, 78, 69]\n",
            "Layer 1: Active tokens per batch = [83, 76, 79, 83, 76, 77, 67, 80, 67, 76, 74, 78, 78, 67, 78, 69]\n",
            "Layer 0: Active tokens per batch = [70, 71, 76, 74, 79, 77, 74, 71, 77, 74, 71, 76, 82, 75, 80, 77]\n",
            "Layer 1: Active tokens per batch = [70, 71, 76, 74, 79, 77, 74, 71, 77, 74, 71, 76, 82, 75, 80, 77]\n",
            "Layer 0: Active tokens per batch = [69, 81, 78, 69, 76, 82, 73, 79, 75, 69, 75, 80, 84, 72, 71, 74]\n",
            "Layer 1: Active tokens per batch = [69, 81, 78, 69, 76, 82, 73, 79, 75, 69, 75, 80, 84, 72, 71, 74]\n",
            "Layer 0: Active tokens per batch = [77, 75, 76, 85, 71, 68, 76, 82, 77, 75, 79, 69, 74, 66, 77, 77]\n",
            "Layer 1: Active tokens per batch = [77, 75, 76, 85, 71, 68, 76, 82, 77, 75, 79, 69, 74, 66, 77, 77]\n",
            "Layer 0: Active tokens per batch = [77, 68, 81, 79, 82, 90, 88, 67, 82, 86, 91, 78, 70, 64, 71, 66]\n",
            "Layer 1: Active tokens per batch = [77, 68, 81, 79, 82, 90, 88, 67, 82, 86, 91, 78, 70, 64, 71, 66]\n",
            "Layer 0: Active tokens per batch = [81, 80, 74, 82, 73, 79, 81, 80, 79, 76, 73, 82, 74, 76, 68, 68]\n",
            "Layer 1: Active tokens per batch = [81, 80, 74, 82, 73, 79, 81, 80, 79, 76, 73, 82, 74, 76, 68, 68]\n",
            "Layer 0: Active tokens per batch = [74, 77, 67, 82, 72, 74, 76, 85, 91, 69, 80, 70, 77, 77, 79, 71]\n",
            "Layer 1: Active tokens per batch = [74, 77, 67, 82, 72, 74, 76, 85, 91, 69, 80, 70, 77, 77, 79, 71]\n",
            "Layer 0: Active tokens per batch = [81, 81, 67, 76, 70, 87, 62, 82, 82, 71, 81, 80, 80, 77, 78, 79]\n",
            "Layer 1: Active tokens per batch = [81, 81, 67, 76, 70, 87, 62, 82, 82, 71, 81, 80, 80, 77, 78, 79]\n",
            "Layer 0: Active tokens per batch = [77, 71, 81, 81, 81, 74, 74, 72, 77, 79, 78, 82, 78, 78, 85, 69]\n",
            "Layer 1: Active tokens per batch = [77, 71, 81, 81, 81, 74, 74, 72, 77, 79, 78, 82, 78, 78, 85, 69]\n",
            "Layer 0: Active tokens per batch = [80, 72, 81, 75, 69, 81, 79, 77, 73, 89, 79, 76, 69, 78, 71, 77]\n",
            "Layer 1: Active tokens per batch = [80, 72, 81, 75, 69, 81, 79, 77, 73, 89, 79, 76, 69, 78, 71, 77]\n",
            "Layer 0: Active tokens per batch = [82, 80, 77, 81, 73, 76, 84, 82, 75, 84, 81, 68, 86, 82, 77, 74]\n",
            "Layer 1: Active tokens per batch = [82, 80, 77, 81, 73, 76, 84, 82, 75, 84, 81, 68, 86, 82, 77, 74]\n",
            "Layer 0: Active tokens per batch = [79, 72, 76, 76, 74, 79, 70, 79, 86, 71, 71, 64, 69, 69, 72, 78]\n",
            "Layer 1: Active tokens per batch = [79, 72, 76, 76, 74, 79, 70, 79, 86, 71, 71, 64, 69, 69, 72, 78]\n",
            "Layer 0: Active tokens per batch = [82, 74, 85, 82, 71, 74, 73, 80, 73, 71, 76, 82, 77, 76, 67, 73]\n",
            "Layer 1: Active tokens per batch = [82, 74, 85, 82, 71, 74, 73, 80, 73, 71, 76, 82, 77, 76, 67, 73]\n",
            "Layer 0: Active tokens per batch = [73, 70, 74, 86, 74, 75, 70, 77, 83, 83, 75, 68, 80, 77, 72, 75]\n",
            "Layer 1: Active tokens per batch = [73, 70, 74, 86, 74, 75, 70, 77, 83, 83, 75, 68, 80, 77, 72, 75]\n",
            "Layer 0: Active tokens per batch = [80, 79, 73, 79, 72, 69, 71, 79, 78, 77, 79, 72, 79, 70, 74, 67]\n",
            "Layer 1: Active tokens per batch = [80, 79, 73, 79, 72, 69, 71, 79, 78, 77, 79, 72, 79, 70, 74, 67]\n",
            "Layer 0: Active tokens per batch = [80, 69, 69, 85, 67, 79, 79, 71, 63, 81, 76, 84, 84, 77, 69, 79]\n",
            "Layer 1: Active tokens per batch = [80, 69, 69, 85, 67, 79, 79, 71, 63, 81, 76, 84, 84, 77, 69, 79]\n",
            "Layer 0: Active tokens per batch = [75, 77, 76, 79, 66, 68, 86, 71, 72, 73, 80, 77, 75, 76, 86, 82]\n",
            "Layer 1: Active tokens per batch = [75, 77, 76, 79, 66, 68, 86, 71, 72, 73, 80, 77, 75, 76, 86, 82]\n",
            "Layer 0: Active tokens per batch = [71, 74, 70, 75, 78, 73, 70, 74, 75, 80, 76, 73, 80, 79, 76, 68]\n",
            "Layer 1: Active tokens per batch = [71, 74, 70, 75, 78, 73, 70, 74, 75, 80, 76, 73, 80, 79, 76, 68]\n",
            "Layer 0: Active tokens per batch = [80, 75, 74, 70, 78, 71, 80, 78, 72, 80, 79, 80, 68, 78, 79, 74]\n",
            "Layer 1: Active tokens per batch = [80, 75, 74, 70, 78, 71, 80, 78, 72, 80, 79, 80, 68, 78, 79, 74]\n",
            "Layer 0: Active tokens per batch = [77, 78, 78, 73, 73, 67, 76, 84, 78, 74, 69, 80, 72, 79, 82, 75]\n",
            "Layer 1: Active tokens per batch = [77, 78, 78, 73, 73, 67, 76, 84, 78, 74, 69, 80, 72, 79, 82, 75]\n",
            "Layer 0: Active tokens per batch = [82, 79, 72, 79, 66, 81, 76, 79, 71, 77, 74, 79, 73, 76, 77, 68]\n",
            "Layer 1: Active tokens per batch = [82, 79, 72, 79, 66, 81, 76, 79, 71, 77, 74, 79, 73, 76, 77, 68]\n",
            "Layer 0: Active tokens per batch = [75, 81, 72, 77, 76, 81, 85, 77, 77, 81, 76, 75, 77, 76, 64, 69]\n",
            "Layer 1: Active tokens per batch = [75, 81, 72, 77, 76, 81, 85, 77, 77, 81, 76, 75, 77, 76, 64, 69]\n",
            "Layer 0: Active tokens per batch = [70, 77, 76, 70, 66, 73, 71, 74, 86, 77, 84, 83, 81, 78, 88, 72]\n",
            "Layer 1: Active tokens per batch = [70, 77, 76, 70, 66, 73, 71, 74, 86, 77, 84, 83, 81, 78, 88, 72]\n",
            "Layer 0: Active tokens per batch = [69, 83, 80, 74, 85, 71, 70, 72, 69, 76, 76, 82, 68, 70, 80, 80]\n",
            "Layer 1: Active tokens per batch = [69, 83, 80, 74, 85, 71, 70, 72, 69, 76, 76, 82, 68, 70, 80, 80]\n",
            "Layer 0: Active tokens per batch = [76, 87, 79, 73, 78, 82, 78, 75, 67, 76, 72, 74, 75, 82, 82, 77]\n",
            "Layer 1: Active tokens per batch = [76, 87, 79, 73, 78, 82, 78, 75, 67, 76, 72, 74, 75, 82, 82, 77]\n",
            "Layer 0: Active tokens per batch = [84, 68, 79, 77, 96, 68, 80, 85, 86, 79, 80, 78, 71, 72, 82, 73]\n",
            "Layer 1: Active tokens per batch = [84, 68, 79, 77, 96, 68, 80, 85, 86, 79, 80, 78, 71, 72, 82, 73]\n",
            "Layer 0: Active tokens per batch = [94, 70, 82, 78, 72, 74, 75, 79, 77, 87, 74, 77, 79, 73, 64, 80]\n",
            "Layer 1: Active tokens per batch = [94, 70, 82, 78, 72, 74, 75, 79, 77, 87, 74, 77, 79, 73, 64, 80]\n",
            "Layer 0: Active tokens per batch = [65, 82, 79, 78, 74, 74, 68, 72, 71, 83, 77, 76, 83, 79, 69, 84]\n",
            "Layer 1: Active tokens per batch = [65, 82, 79, 78, 74, 74, 68, 72, 71, 83, 77, 76, 83, 79, 69, 84]\n",
            "Layer 0: Active tokens per batch = [67, 81, 89, 81, 67, 77, 71, 78, 75, 78, 83, 84, 71, 77, 77, 79]\n",
            "Layer 1: Active tokens per batch = [67, 81, 89, 81, 67, 77, 71, 78, 75, 78, 83, 84, 71, 77, 77, 79]\n",
            "Layer 0: Active tokens per batch = [75, 64, 76, 68, 61, 74, 86, 72]\n",
            "Layer 1: Active tokens per batch = [75, 64, 76, 68, 61, 74, 86, 72]\n",
            "Layer 0: Active tokens per batch = [81, 91, 89, 79, 79, 76, 81, 75, 82, 75, 77, 74, 84, 81, 72, 72]\n",
            "Layer 1: Active tokens per batch = [81, 91, 89, 79, 79, 76, 81, 75, 82, 75, 77, 74, 84, 81, 72, 72]\n",
            "Layer 0: Active tokens per batch = [83, 86, 67, 68, 73, 74, 70, 67, 70, 75, 68, 87, 81, 75, 71, 79]\n",
            "Layer 1: Active tokens per batch = [83, 86, 67, 68, 73, 74, 70, 67, 70, 75, 68, 87, 81, 75, 71, 79]\n",
            "Layer 0: Active tokens per batch = [81, 79, 85, 80, 75, 83, 83, 82, 72, 80, 69, 69, 80, 67, 84, 71]\n",
            "Layer 1: Active tokens per batch = [81, 79, 85, 80, 75, 83, 83, 82, 72, 80, 69, 69, 80, 67, 84, 71]\n",
            "Layer 0: Active tokens per batch = [76, 83, 79, 89, 71, 75, 81, 80, 81, 78, 73, 83, 74, 77, 76, 79]\n",
            "Layer 1: Active tokens per batch = [76, 83, 79, 89, 71, 75, 81, 80, 81, 78, 73, 83, 74, 77, 76, 79]\n",
            "Layer 0: Active tokens per batch = [75, 81, 82, 79, 75, 81, 75, 74, 72, 75, 76, 66, 76, 74, 72, 69]\n",
            "Layer 1: Active tokens per batch = [75, 81, 82, 79, 75, 81, 75, 74, 72, 75, 76, 66, 76, 74, 72, 69]\n",
            "Layer 0: Active tokens per batch = [73, 75, 68, 78, 80, 70, 78, 82, 80, 76, 64, 71, 67, 87, 83, 79]\n",
            "Layer 1: Active tokens per batch = [73, 75, 68, 78, 80, 70, 78, 82, 80, 76, 64, 71, 67, 87, 83, 79]\n",
            "Layer 0: Active tokens per batch = [79, 79, 79, 70, 68, 74, 77, 83, 74, 77, 68, 80, 69, 71, 74, 77]\n",
            "Layer 1: Active tokens per batch = [79, 79, 79, 70, 68, 74, 77, 83, 74, 77, 68, 80, 69, 71, 74, 77]\n",
            "Layer 0: Active tokens per batch = [71, 83, 73, 78, 75, 86, 82, 83, 81, 67, 73, 71, 80, 81, 73, 75]\n",
            "Layer 1: Active tokens per batch = [71, 83, 73, 78, 75, 86, 82, 83, 81, 67, 73, 71, 80, 81, 73, 75]\n",
            "Layer 0: Active tokens per batch = [72, 78, 84, 80, 79, 81, 84, 72, 70, 81, 77, 84, 82, 81, 79, 75]\n",
            "Layer 1: Active tokens per batch = [72, 78, 84, 80, 79, 81, 84, 72, 70, 81, 77, 84, 82, 81, 79, 75]\n",
            "Layer 0: Active tokens per batch = [79, 80, 70, 73, 79, 72, 75, 69, 71, 73, 70, 79, 82, 74, 88, 76]\n",
            "Layer 1: Active tokens per batch = [79, 80, 70, 73, 79, 72, 75, 69, 71, 73, 70, 79, 82, 74, 88, 76]\n",
            "Layer 0: Active tokens per batch = [73, 79, 84, 78, 78, 74, 74, 74, 69, 69, 80, 77, 73, 71, 68, 74]\n",
            "Layer 1: Active tokens per batch = [73, 79, 84, 78, 78, 74, 74, 74, 69, 69, 80, 77, 73, 71, 68, 74]\n",
            "Layer 0: Active tokens per batch = [70, 73, 70, 78, 68, 77, 64, 71, 67, 75, 70, 76, 64, 77, 75, 71]\n",
            "Layer 1: Active tokens per batch = [70, 73, 70, 78, 68, 77, 64, 71, 67, 75, 70, 76, 64, 77, 75, 71]\n",
            "Layer 0: Active tokens per batch = [76, 78, 76, 70, 87, 68, 86, 75, 84, 84, 82, 77, 68, 77, 75, 84]\n",
            "Layer 1: Active tokens per batch = [76, 78, 76, 70, 87, 68, 86, 75, 84, 84, 82, 77, 68, 77, 75, 84]\n",
            "Layer 0: Active tokens per batch = [80, 84, 79, 73, 85, 87, 77, 78, 76, 70, 82, 72, 76, 79, 78, 80]\n",
            "Layer 1: Active tokens per batch = [80, 84, 79, 73, 85, 87, 77, 78, 76, 70, 82, 72, 76, 79, 78, 80]\n",
            "Layer 0: Active tokens per batch = [78, 86, 73, 71, 76, 80, 86, 75, 77, 78, 74, 71, 76, 81, 78, 78]\n",
            "Layer 1: Active tokens per batch = [78, 86, 73, 71, 76, 80, 86, 75, 77, 78, 74, 71, 76, 81, 78, 78]\n",
            "Layer 0: Active tokens per batch = [79, 75, 84, 74, 78, 70, 78, 80, 72, 82, 76, 77, 78, 77, 80, 76]\n",
            "Layer 1: Active tokens per batch = [79, 75, 84, 74, 78, 70, 78, 80, 72, 82, 76, 77, 78, 77, 80, 76]\n",
            "Layer 0: Active tokens per batch = [82, 78, 77, 76, 72, 76, 72, 71, 77, 75, 74, 77, 78, 74, 77, 69]\n",
            "Layer 1: Active tokens per batch = [82, 78, 77, 76, 72, 76, 72, 71, 77, 75, 74, 77, 78, 74, 77, 69]\n",
            "Layer 0: Active tokens per batch = [77, 79, 80, 78, 71, 86, 75, 82, 81, 73, 78, 75, 74, 62, 69, 79]\n",
            "Layer 1: Active tokens per batch = [77, 79, 80, 78, 71, 86, 75, 82, 81, 73, 78, 75, 74, 62, 69, 79]\n",
            "Layer 0: Active tokens per batch = [77, 77, 78, 74, 71, 85, 82, 82, 77, 72, 69, 71, 86, 79, 73, 84]\n",
            "Layer 1: Active tokens per batch = [77, 77, 78, 74, 71, 85, 82, 82, 77, 72, 69, 71, 86, 79, 73, 84]\n",
            "Layer 0: Active tokens per batch = [74, 65, 79, 72, 74, 76, 80, 81, 85, 75, 79, 75, 74, 81, 80, 71]\n",
            "Layer 1: Active tokens per batch = [74, 65, 79, 72, 74, 76, 80, 81, 85, 75, 79, 75, 74, 81, 80, 71]\n",
            "Layer 0: Active tokens per batch = [65, 69, 77, 74, 74, 78, 78, 79, 70, 73, 77, 78, 85, 71, 77, 74]\n",
            "Layer 1: Active tokens per batch = [65, 69, 77, 74, 74, 78, 78, 79, 70, 73, 77, 78, 85, 71, 77, 74]\n",
            "Layer 0: Active tokens per batch = [71, 75, 74, 75, 72, 79, 68, 77, 76, 79, 80, 76, 75, 75, 70, 79]\n",
            "Layer 1: Active tokens per batch = [71, 75, 74, 75, 72, 79, 68, 77, 76, 79, 80, 76, 75, 75, 70, 79]\n",
            "Layer 0: Active tokens per batch = [66, 72, 77, 78, 68, 84, 74, 77, 76, 75, 78, 88, 75, 70, 74, 80]\n",
            "Layer 1: Active tokens per batch = [66, 72, 77, 78, 68, 84, 74, 77, 76, 75, 78, 88, 75, 70, 74, 80]\n",
            "Layer 0: Active tokens per batch = [74, 78, 71, 77, 84, 70, 68, 64, 70, 78, 78, 75, 73, 79, 79, 84]\n",
            "Layer 1: Active tokens per batch = [74, 78, 71, 77, 84, 70, 68, 64, 70, 78, 78, 75, 73, 79, 79, 84]\n",
            "Layer 0: Active tokens per batch = [76, 66, 73, 76, 69, 69, 96, 80, 79, 77, 72, 77, 76, 78, 79, 79]\n",
            "Layer 1: Active tokens per batch = [76, 66, 73, 76, 69, 69, 96, 80, 79, 77, 72, 77, 76, 78, 79, 79]\n",
            "Layer 0: Active tokens per batch = [71, 70, 75, 76, 82, 72, 76, 72, 85, 74, 78, 77, 76, 78, 79, 72]\n",
            "Layer 1: Active tokens per batch = [71, 70, 75, 76, 82, 72, 76, 72, 85, 74, 78, 77, 76, 78, 79, 72]\n",
            "Layer 0: Active tokens per batch = [76, 68, 76, 87, 71, 75, 73, 79, 72, 66, 81, 79, 70, 85, 85, 79]\n",
            "Layer 1: Active tokens per batch = [76, 68, 76, 87, 71, 75, 73, 79, 72, 66, 81, 79, 70, 85, 85, 79]\n",
            "Layer 0: Active tokens per batch = [79, 67, 83, 77, 71, 72, 86, 75, 56, 63, 78, 84, 70, 71, 78, 76]\n",
            "Layer 1: Active tokens per batch = [79, 67, 83, 77, 71, 72, 86, 75, 56, 63, 78, 84, 70, 71, 78, 76]\n",
            "Layer 0: Active tokens per batch = [66, 72, 80, 74, 83, 77, 74, 72, 76, 85, 83, 66, 74, 69, 84, 73]\n",
            "Layer 1: Active tokens per batch = [66, 72, 80, 74, 83, 77, 74, 72, 76, 85, 83, 66, 74, 69, 84, 73]\n",
            "Layer 0: Active tokens per batch = [76, 79, 73, 78, 74, 78, 77, 77, 77, 82, 78, 75, 79, 75, 76, 72]\n",
            "Layer 1: Active tokens per batch = [76, 79, 73, 78, 74, 78, 77, 77, 77, 82, 78, 75, 79, 75, 76, 72]\n",
            "Layer 0: Active tokens per batch = [78, 74, 68, 67, 82, 78, 79, 83, 67, 84, 76, 80, 73, 75, 72, 78]\n",
            "Layer 1: Active tokens per batch = [78, 74, 68, 67, 82, 78, 79, 83, 67, 84, 76, 80, 73, 75, 72, 78]\n",
            "Layer 0: Active tokens per batch = [71, 77, 75, 79, 75, 87, 71, 79, 72, 86, 72, 82, 87, 82, 79, 70]\n",
            "Layer 1: Active tokens per batch = [71, 77, 75, 79, 75, 87, 71, 79, 72, 86, 72, 82, 87, 82, 79, 70]\n",
            "Layer 0: Active tokens per batch = [73, 80, 76, 71, 94, 75, 74, 67, 74, 83, 74, 75, 64, 74, 65, 81]\n",
            "Layer 1: Active tokens per batch = [73, 80, 76, 71, 94, 75, 74, 67, 74, 83, 74, 75, 64, 74, 65, 81]\n",
            "Layer 0: Active tokens per batch = [78, 67, 71, 82, 73, 72, 72, 79, 81, 76, 75, 86, 78, 76, 81, 77]\n",
            "Layer 1: Active tokens per batch = [78, 67, 71, 82, 73, 72, 72, 79, 81, 76, 75, 86, 78, 76, 81, 77]\n",
            "Layer 0: Active tokens per batch = [70, 74, 75, 76, 76, 76, 71, 77, 69, 68, 75, 67, 81, 69, 85, 75]\n",
            "Layer 1: Active tokens per batch = [70, 74, 75, 76, 76, 76, 71, 77, 69, 68, 75, 67, 81, 69, 85, 75]\n",
            "Layer 0: Active tokens per batch = [71, 86, 78, 76, 83, 82, 68, 73, 77, 80, 70, 76, 82, 81, 75, 71]\n",
            "Layer 1: Active tokens per batch = [71, 86, 78, 76, 83, 82, 68, 73, 77, 80, 70, 76, 82, 81, 75, 71]\n",
            "Layer 0: Active tokens per batch = [70, 80, 76, 82, 85, 71, 77, 80, 79, 68, 79, 77, 73, 76, 75, 76]\n",
            "Layer 1: Active tokens per batch = [70, 80, 76, 82, 85, 71, 77, 80, 79, 68, 79, 77, 73, 76, 75, 76]\n",
            "Layer 0: Active tokens per batch = [73, 82, 76, 86, 80, 67, 85, 69, 72, 71, 81, 75, 79, 73, 80, 74]\n",
            "Layer 1: Active tokens per batch = [73, 82, 76, 86, 80, 67, 85, 69, 72, 71, 81, 75, 79, 73, 80, 74]\n",
            "Layer 0: Active tokens per batch = [74, 85, 80, 70, 65, 69, 65, 74, 74, 78, 77, 74, 69, 78, 75, 86]\n",
            "Layer 1: Active tokens per batch = [74, 85, 80, 70, 65, 69, 65, 74, 74, 78, 77, 74, 69, 78, 75, 86]\n",
            "Layer 0: Active tokens per batch = [68, 77, 80, 85, 77, 87, 73, 80, 78, 77, 85, 75, 80, 75, 69, 77]\n",
            "Layer 1: Active tokens per batch = [68, 77, 80, 85, 77, 87, 73, 80, 78, 77, 85, 75, 80, 75, 69, 77]\n",
            "Layer 0: Active tokens per batch = [71, 76, 79, 80, 69, 78, 84, 82, 68, 71, 86, 81, 70, 82, 73, 71]\n",
            "Layer 1: Active tokens per batch = [71, 76, 79, 80, 69, 78, 84, 82, 68, 71, 86, 81, 70, 82, 73, 71]\n",
            "Layer 0: Active tokens per batch = [80, 75, 78, 71, 76, 82, 79, 78, 74, 77, 61, 77, 74, 66, 76, 74]\n",
            "Layer 1: Active tokens per batch = [80, 75, 78, 71, 76, 82, 79, 78, 74, 77, 61, 77, 74, 66, 76, 74]\n",
            "Layer 0: Active tokens per batch = [70, 72, 88, 84, 69, 79, 74, 78, 73, 78, 74, 80, 81, 86, 69, 81]\n",
            "Layer 1: Active tokens per batch = [70, 72, 88, 84, 69, 79, 74, 78, 73, 78, 74, 80, 81, 86, 69, 81]\n",
            "Layer 0: Active tokens per batch = [74, 82, 85, 86, 75, 76, 80, 73, 75, 70, 90, 82, 69, 71, 79, 83]\n",
            "Layer 1: Active tokens per batch = [74, 82, 85, 86, 75, 76, 80, 73, 75, 70, 90, 82, 69, 71, 79, 83]\n",
            "Layer 0: Active tokens per batch = [71, 80, 85, 67, 80, 73, 74, 80, 70, 82, 68, 75, 78, 74, 78, 82]\n",
            "Layer 1: Active tokens per batch = [71, 80, 85, 67, 80, 73, 74, 80, 70, 82, 68, 75, 78, 74, 78, 82]\n",
            "Layer 0: Active tokens per batch = [79, 78, 69, 65, 78, 81, 76, 83, 77, 76, 77, 78, 82, 91, 79, 78]\n",
            "Layer 1: Active tokens per batch = [79, 78, 69, 65, 78, 81, 76, 83, 77, 76, 77, 78, 82, 91, 79, 78]\n",
            "Layer 0: Active tokens per batch = [77, 67, 81, 84, 69, 66, 77, 74, 73, 73, 70, 84, 71, 79, 77, 78]\n",
            "Layer 1: Active tokens per batch = [77, 67, 81, 84, 69, 66, 77, 74, 73, 73, 70, 84, 71, 79, 77, 78]\n",
            "Layer 0: Active tokens per batch = [80, 70, 80, 72, 70, 68, 81, 81, 75, 76, 86, 85, 79, 67, 70, 74]\n",
            "Layer 1: Active tokens per batch = [80, 70, 80, 72, 70, 68, 81, 81, 75, 76, 86, 85, 79, 67, 70, 74]\n",
            "Layer 0: Active tokens per batch = [72, 76, 73, 66, 67, 78, 76, 71, 75, 80, 75, 66, 78, 82, 69, 78]\n",
            "Layer 1: Active tokens per batch = [72, 76, 73, 66, 67, 78, 76, 71, 75, 80, 75, 66, 78, 82, 69, 78]\n",
            "Layer 0: Active tokens per batch = [81, 80, 71, 77, 86, 72, 77, 83, 74, 79, 67, 79, 74, 78, 80, 79]\n",
            "Layer 1: Active tokens per batch = [81, 80, 71, 77, 86, 72, 77, 83, 74, 79, 67, 79, 74, 78, 80, 79]\n",
            "Layer 0: Active tokens per batch = [81, 81, 83, 72, 84, 82, 81, 82, 69, 82, 74, 73, 78, 70, 71, 85]\n",
            "Layer 1: Active tokens per batch = [81, 81, 83, 72, 84, 82, 81, 82, 69, 82, 74, 73, 78, 70, 71, 85]\n",
            "Layer 0: Active tokens per batch = [67, 82, 64, 78, 69, 75, 72, 72, 73, 73, 81, 70, 77, 80, 83, 69]\n",
            "Layer 1: Active tokens per batch = [67, 82, 64, 78, 69, 75, 72, 72, 73, 73, 81, 70, 77, 80, 83, 69]\n",
            "Layer 0: Active tokens per batch = [80, 85, 74, 80, 82, 80, 85, 69, 73, 77, 73, 72, 82, 82, 75, 71]\n",
            "Layer 1: Active tokens per batch = [80, 85, 74, 80, 82, 80, 85, 69, 73, 77, 73, 72, 82, 82, 75, 71]\n",
            "Layer 0: Active tokens per batch = [82, 72, 71, 80, 78, 79, 81, 78, 80, 82, 81, 84, 65, 80, 77, 91]\n",
            "Layer 1: Active tokens per batch = [82, 72, 71, 80, 78, 79, 81, 78, 80, 82, 81, 84, 65, 80, 77, 91]\n",
            "Layer 0: Active tokens per batch = [82, 67, 79, 78, 76, 68, 82, 77, 64, 74, 85, 75, 87, 73, 70, 72]\n",
            "Layer 1: Active tokens per batch = [82, 67, 79, 78, 76, 68, 82, 77, 64, 74, 85, 75, 87, 73, 70, 72]\n",
            "Layer 0: Active tokens per batch = [64, 80, 87, 64, 86, 71, 77, 75, 75, 83, 75, 73, 77, 79, 72, 78]\n",
            "Layer 1: Active tokens per batch = [64, 80, 87, 64, 86, 71, 77, 75, 75, 83, 75, 73, 77, 79, 72, 78]\n",
            "Layer 0: Active tokens per batch = [80, 72, 71, 76, 73, 77, 73, 80, 78, 76, 71, 69, 73, 73, 73, 74]\n",
            "Layer 1: Active tokens per batch = [80, 72, 71, 76, 73, 77, 73, 80, 78, 76, 71, 69, 73, 73, 73, 74]\n",
            "Layer 0: Active tokens per batch = [80, 72, 74, 67, 79, 80, 84, 76, 81, 74, 83, 63, 67, 73, 75, 77]\n",
            "Layer 1: Active tokens per batch = [80, 72, 74, 67, 79, 80, 84, 76, 81, 74, 83, 63, 67, 73, 75, 77]\n",
            "Layer 0: Active tokens per batch = [77, 86, 85, 79, 76, 76, 80, 82, 69, 77, 80, 76, 70, 84, 77, 68]\n",
            "Layer 1: Active tokens per batch = [77, 86, 85, 79, 76, 76, 80, 82, 69, 77, 80, 76, 70, 84, 77, 68]\n",
            "Layer 0: Active tokens per batch = [79, 79, 72, 66, 67, 88, 78, 74, 62, 82, 80, 73, 84, 76, 81, 78]\n",
            "Layer 1: Active tokens per batch = [79, 79, 72, 66, 67, 88, 78, 74, 62, 82, 80, 73, 84, 76, 81, 78]\n",
            "Layer 0: Active tokens per batch = [66, 68, 77, 80, 74, 84, 79, 74, 74, 78, 78, 78, 82, 73, 81, 71]\n",
            "Layer 1: Active tokens per batch = [66, 68, 77, 80, 74, 84, 79, 74, 74, 78, 78, 78, 82, 73, 81, 71]\n",
            "Layer 0: Active tokens per batch = [74, 75, 70, 69, 83, 82, 77, 72, 70, 73, 74, 82, 63, 67, 76, 64]\n",
            "Layer 1: Active tokens per batch = [74, 75, 70, 69, 83, 82, 77, 72, 70, 73, 74, 82, 63, 67, 76, 64]\n",
            "Layer 0: Active tokens per batch = [76, 80, 77, 72, 79, 71, 72, 74]\n",
            "Layer 1: Active tokens per batch = [76, 80, 77, 72, 79, 71, 72, 74]\n",
            "Layer 0: Active tokens per batch = [78, 72, 75, 76, 78, 78, 72, 78, 73, 71, 78, 80, 65, 72, 75, 79]\n",
            "Layer 1: Active tokens per batch = [78, 72, 75, 76, 78, 78, 72, 78, 73, 71, 78, 80, 65, 72, 75, 79]\n",
            "Layer 0: Active tokens per batch = [82, 66, 79, 72, 76, 82, 86, 72, 65, 87, 79, 80, 77, 78, 87, 68]\n",
            "Layer 1: Active tokens per batch = [82, 66, 79, 72, 76, 82, 86, 72, 65, 87, 79, 80, 77, 78, 87, 68]\n",
            "Layer 0: Active tokens per batch = [76, 74, 74, 81, 76, 76, 74, 80, 80, 82, 72, 75, 79, 84, 69, 75]\n",
            "Layer 1: Active tokens per batch = [76, 74, 74, 81, 76, 76, 74, 80, 80, 82, 72, 75, 79, 84, 69, 75]\n",
            "Layer 0: Active tokens per batch = [67, 81, 58, 71, 81, 82, 83, 69, 75, 81, 76, 74, 81, 80, 85, 70]\n",
            "Layer 1: Active tokens per batch = [67, 81, 58, 71, 81, 82, 83, 69, 75, 81, 76, 74, 81, 80, 85, 70]\n",
            "Layer 0: Active tokens per batch = [80, 78, 71, 78, 69, 77, 75, 82, 61, 67, 82, 78, 69, 77, 80, 78]\n",
            "Layer 1: Active tokens per batch = [80, 78, 71, 78, 69, 77, 75, 82, 61, 67, 82, 78, 69, 77, 80, 78]\n",
            "Layer 0: Active tokens per batch = [73, 74, 83, 68, 74, 81, 69, 77, 78, 71, 78, 77, 74, 82, 70, 79]\n",
            "Layer 1: Active tokens per batch = [73, 74, 83, 68, 74, 81, 69, 77, 78, 71, 78, 77, 74, 82, 70, 79]\n",
            "Layer 0: Active tokens per batch = [71, 76, 75, 74, 79, 72, 78, 82, 80, 72, 85, 83, 86, 78, 68, 80]\n",
            "Layer 1: Active tokens per batch = [71, 76, 75, 74, 79, 72, 78, 82, 80, 72, 85, 83, 86, 78, 68, 80]\n",
            "Layer 0: Active tokens per batch = [63, 74, 76, 76, 76, 82, 82, 65, 77, 84, 75, 78, 73, 81, 82, 73]\n",
            "Layer 1: Active tokens per batch = [63, 74, 76, 76, 76, 82, 82, 65, 77, 84, 75, 78, 73, 81, 82, 73]\n",
            "Layer 0: Active tokens per batch = [72, 72, 81, 71, 73, 66, 82, 92, 69, 68, 80, 78, 73, 72, 69, 74]\n",
            "Layer 1: Active tokens per batch = [72, 72, 81, 71, 73, 66, 82, 92, 69, 68, 80, 78, 73, 72, 69, 74]\n",
            "Layer 0: Active tokens per batch = [66, 72, 72, 74, 78, 68, 66, 83, 74, 77, 78, 81, 74, 79, 64, 78]\n",
            "Layer 1: Active tokens per batch = [66, 72, 72, 74, 78, 68, 66, 83, 74, 77, 78, 81, 74, 79, 64, 78]\n",
            "Layer 0: Active tokens per batch = [67, 76, 67, 78, 77, 75, 80, 73, 79, 76, 81, 74, 82, 76, 74, 74]\n",
            "Layer 1: Active tokens per batch = [67, 76, 67, 78, 77, 75, 80, 73, 79, 76, 81, 74, 82, 76, 74, 74]\n",
            "Layer 0: Active tokens per batch = [76, 86, 79, 71, 77, 82, 69, 67, 75, 73, 68, 80, 73, 84, 73, 78]\n",
            "Layer 1: Active tokens per batch = [76, 86, 79, 71, 77, 82, 69, 67, 75, 73, 68, 80, 73, 84, 73, 78]\n",
            "Layer 0: Active tokens per batch = [82, 74, 65, 77, 70, 63, 77, 72]\n",
            "Layer 1: Active tokens per batch = [82, 74, 65, 77, 70, 63, 77, 72]\n",
            "Accuracy: 47.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.FLOPs versus memory performance"
      ],
      "metadata": {
        "id": "Ebku0hJaD6Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.profiler import profile, ProfilerActivity\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "\n",
        "def calculate_dynamic_flops_and_profile(pruned_model, input_tensor):\n",
        "    \"\"\"\n",
        "    Calculate dynamic FLOPs and memory usage for the pruned model.\n",
        "    Args:\n",
        "        pruned_model: Model with dynamic token pruning.\n",
        "        input_tensor: Input tensor.\n",
        "    \"\"\"\n",
        "    # Dynamic computing FLOPs\n",
        "    print(\"\\n=== Pruned Model ===\")\n",
        "    flops_pruned = FlopCountAnalysis(pruned_model, input_tensor)\n",
        "    print(flop_count_table(flops_pruned))\n",
        "\n",
        "    # Dynamic profile memory and time\n",
        "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
        "        _ = pruned_model(input_tensor)\n",
        "    print(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n",
        "\n",
        "def compare_efficiency(baseline_model, pruned_model):\n",
        "    \"\"\"\n",
        "    Compare FLOPs and memory usage for baseline and pruned models.\n",
        "    Args:\n",
        "        baseline_model, pruned_model: Models to compare.\n",
        "    \"\"\"\n",
        "    input_tensor = torch.rand(16, 128, 512).cuda()  # Simulated input: batch size=16, tokens=128, dim=512\n",
        "\n",
        "    # FLOPs and performance evaluation of Baseline Model\n",
        "    print(\"\\n=== Baseline Model ===\")\n",
        "    flops_baseline = FlopCountAnalysis(baseline_model, input_tensor)\n",
        "    print(flop_count_table(flops_baseline))\n",
        "    profile_memory_and_time_safe(baseline_model, input_tensor)\n",
        "\n",
        "    # Pruned Model dynamic FLOPs and performance evaluation\n",
        "    calculate_dynamic_flops_and_profile(pruned_model, input_tensor)\n",
        "\n"
      ],
      "metadata": {
        "id": "B3gCYwORELTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_efficiency(baseline_model, pruned_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIAyp4w2EPJO",
        "outputId": "a1f2f288-f2f6-4b9f-ed3a-6b7531f17c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Baseline Model ===\n",
            "| module                  | #parameters or shape   | #flops    |\n",
            "|:------------------------|:-----------------------|:----------|\n",
            "| model                   | 6.306M                 | 12.908G   |\n",
            "|  0.layers               |  6.305M                |  12.906G  |\n",
            "|   0.layers.0            |   3.152M               |   6.453G  |\n",
            "|    0.layers.0.self_attn |    1.051M              |    2.147G |\n",
            "|    0.layers.0.linear1   |    1.051M              |    2.147G |\n",
            "|    0.layers.0.linear2   |    1.049M              |    2.147G |\n",
            "|    0.layers.0.norm1     |    1.024K              |    5.243M |\n",
            "|    0.layers.0.norm2     |    1.024K              |    5.243M |\n",
            "|   0.layers.1            |   3.152M               |   6.453G  |\n",
            "|    0.layers.1.self_attn |    1.051M              |    2.147G |\n",
            "|    0.layers.1.linear1   |    1.051M              |    2.147G |\n",
            "|    0.layers.1.linear2   |    1.049M              |    2.147G |\n",
            "|    0.layers.1.norm1     |    1.024K              |    5.243M |\n",
            "|    0.layers.1.norm2     |    1.024K              |    5.243M |\n",
            "|  1.fc                   |  1.026K                |  2.097M   |\n",
            "|   1.fc.weight           |   (2, 512)             |           |\n",
            "|   1.fc.bias             |   (2,)                 |           |\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::linear         0.73%      98.001us        17.34%       2.315ms     257.199us       0.000us         0.00%      10.207ms       1.134ms           0 b           0 b      72.02 Mb           0 b             9  \n",
            "                                            aten::addmm         8.67%       1.158ms        15.36%       2.051ms     227.905us      10.207ms        81.72%      10.207ms       1.134ms           0 b           0 b      72.02 Mb      72.02 Mb             9  \n",
            "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us       7.713ms        61.75%       7.713ms       1.285ms           0 b           0 b           0 b           0 b             6  \n",
            "                                 volta_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us       2.424ms        19.41%       2.424ms       1.212ms           0 b           0 b           0 b           0 b             2  \n",
            "                     aten::scaled_dot_product_attention         0.25%      32.870us         1.41%     187.961us      93.980us       0.000us         0.00%       1.175ms     587.532us          32 b           0 b       8.25 Mb           0 b             2  \n",
            "          aten::_scaled_dot_product_efficient_attention         0.27%      36.249us         1.16%     155.091us      77.545us       0.000us         0.00%       1.175ms     587.532us          32 b           0 b       8.25 Mb           0 b             2  \n",
            "                     aten::_efficient_attention_forward         0.29%      38.116us         0.74%      98.500us      49.250us       1.175ms         9.41%       1.175ms     587.532us          32 b           0 b       8.25 Mb           0 b             2  \n",
            "fmha_cutlassF_f32_aligned_64x64_rf_sm75(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us       1.175ms         9.41%       1.175ms     587.532us           0 b           0 b           0 b           0 b             2  \n",
            "                                       aten::contiguous         0.06%       7.930us         1.36%     181.123us      45.281us       0.000us         0.00%     327.574us      81.894us           0 b           0 b      32.00 Mb           0 b             4  \n",
            "                                            aten::clone         0.20%      26.052us         1.30%     173.193us      43.298us       0.000us         0.00%     327.574us      81.894us           0 b           0 b      32.00 Mb           0 b             4  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 13.350ms\n",
            "Self CUDA time total: 12.490ms\n",
            "\n",
            "\n",
            "=== Pruned Model ===\n",
            "Layer 0: Active tokens per batch = [81, 74, 82, 74, 73, 73, 64, 76, 79, 77, 79, 80, 88, 76, 77, 79]\n",
            "Layer 1: Active tokens per batch = [81, 74, 82, 74, 73, 73, 64, 76, 79, 77, 79, 80, 88, 76, 77, 79]\n",
            "| module                  | #parameters or shape   | #flops    |\n",
            "|:------------------------|:-----------------------|:----------|\n",
            "| model                   | 6.306M                 | 8.874G    |\n",
            "|  0.layers               |  6.305M                |  8.873G   |\n",
            "|   0.layers.0            |   3.152M               |   4.436G  |\n",
            "|    0.layers.0.self_attn |    1.051M              |    1.476G |\n",
            "|    0.layers.0.linear1   |    1.051M              |    1.476G |\n",
            "|    0.layers.0.linear2   |    1.049M              |    1.476G |\n",
            "|    0.layers.0.norm1     |    1.024K              |    3.604M |\n",
            "|    0.layers.0.norm2     |    1.024K              |    3.604M |\n",
            "|   0.layers.1            |   3.152M               |   4.436G  |\n",
            "|    0.layers.1.self_attn |    1.051M              |    1.476G |\n",
            "|    0.layers.1.linear1   |    1.051M              |    1.476G |\n",
            "|    0.layers.1.linear2   |    1.049M              |    1.476G |\n",
            "|    0.layers.1.norm1     |    1.024K              |    3.604M |\n",
            "|    0.layers.1.norm2     |    1.024K              |    3.604M |\n",
            "|  1.fc                   |  1.026K                |  1.442M   |\n",
            "|   1.fc.weight           |   (2, 512)             |           |\n",
            "|   1.fc.bias             |   (2,)                 |           |\n",
            "Layer 0: Active tokens per batch = [81, 74, 82, 74, 73, 73, 64, 76, 79, 77, 79, 80, 88, 76, 77, 79]\n",
            "Layer 1: Active tokens per batch = [81, 74, 82, 74, 73, 73, 64, 76, 79, 77, 79, 80, 88, 76, 77, 79]\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::linear         0.45%      70.679us         4.71%     742.914us      82.546us       0.000us         0.00%       7.142ms     793.515us             9  \n",
            "                                            aten::addmm         2.22%     351.012us         3.31%     521.614us      57.957us       7.142ms        71.65%       7.142ms     793.515us             9  \n",
            "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us       4.830ms        48.45%       4.830ms     804.935us             6  \n",
            "                                 volta_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us       2.236ms        22.44%       2.236ms       1.118ms             2  \n",
            "                     aten::scaled_dot_product_attention         0.12%      19.010us         1.04%     164.605us      82.303us       0.000us         0.00%     799.207us     399.603us             2  \n",
            "          aten::_scaled_dot_product_efficient_attention         0.28%      43.601us         0.92%     145.595us      72.798us       0.000us         0.00%     799.207us     399.603us             2  \n",
            "                     aten::_efficient_attention_forward         0.19%      30.137us         0.53%      84.153us      42.076us     799.207us         8.02%     799.207us     399.603us             2  \n",
            "fmha_cutlassF_f32_aligned_64x64_rf_sm75(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us     799.207us         8.02%     799.207us     399.603us             2  \n",
            "                                            aten::index         6.61%       1.042ms        10.54%       1.663ms      25.990us     549.970us         5.52%     549.970us       8.593us            64  \n",
            "                                            aten::copy_         2.55%     401.735us         5.70%     899.724us      13.039us     456.562us         4.58%     456.562us       6.617us            69  \n",
            "                                    aten::nonzero_numpy         0.48%      76.394us        37.83%       5.969ms     186.542us       0.000us         0.00%     386.292us      12.072us            32  \n",
            "                                          aten::nonzero         7.26%       1.145ms        36.25%       5.720ms     178.736us     386.292us         3.88%     386.292us      12.072us            32  \n",
            "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     353.303us         3.54%     353.303us      11.041us            32  \n",
            "                                     aten::pad_sequence         1.35%     212.534us        10.08%       1.591ms     397.748us       0.000us         0.00%     249.369us      62.342us             4  \n",
            "                                       aten::contiguous         0.04%       5.682us         1.01%     159.746us      39.937us       0.000us         0.00%     227.801us      56.950us             4  \n",
            "                                            aten::clone         0.13%      20.230us         0.98%     154.064us      38.516us       0.000us         0.00%     227.801us      56.950us             4  \n",
            "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     227.801us         2.29%     227.801us      56.950us             4  \n",
            "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     221.081us         2.22%     221.081us       3.454us            64  \n",
            "                                       aten::layer_norm         0.04%       6.902us         1.24%     196.358us      49.090us       0.000us         0.00%     204.793us      51.198us             4  \n",
            "                                aten::native_layer_norm         0.56%      88.967us         1.20%     189.456us      47.364us     204.793us         2.05%     204.793us      51.198us             4  \n",
            "void at::native::(anonymous namespace)::vectorized_l...         0.00%       0.000us         0.00%       0.000us       0.000us     204.793us         2.05%     204.793us      51.198us             4  \n",
            "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     196.667us         1.97%     196.667us       6.146us            32  \n",
            "                                             aten::relu         0.12%      18.949us         0.45%      70.547us      35.273us       0.000us         0.00%     192.282us      96.141us             2  \n",
            "                                        aten::clamp_min         0.20%      32.206us         0.33%      51.598us      25.799us     192.282us         1.93%     192.282us      96.141us             2  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     192.282us         1.93%     192.282us      96.141us             2  \n",
            "void at_cuda_detail::cub::DeviceSelectSweepKernel<at...         0.00%       0.000us         0.00%       0.000us       0.000us     142.715us         1.43%     142.715us       4.460us            32  \n",
            "                                              aten::add         0.46%      72.621us         0.68%     107.089us      26.772us     140.380us         1.41%     140.380us      35.095us             4  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     140.380us         1.41%     140.380us      35.095us             4  \n",
            "void at_cuda_detail::cub::DeviceReduceSingleTileKern...         0.00%       0.000us         0.00%       0.000us       0.000us     115.262us         1.16%     115.262us       3.602us            32  \n",
            "void at_cuda_detail::cub::DeviceCompactInitKernel<at...         0.00%       0.000us         0.00%       0.000us       0.000us      76.061us         0.76%      76.061us       2.377us            32  \n",
            "                         volta_sgemm_32x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us      61.374us         0.62%      61.374us      61.374us             1  \n",
            "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us      52.254us         0.52%      52.254us       1.633us            32  \n",
            "                               aten::linalg_vector_norm         0.35%      55.798us         0.50%      78.159us      39.080us      51.551us         0.52%      51.551us      25.776us             2  \n",
            "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      51.551us         0.52%      51.551us      25.776us             2  \n",
            "                                            aten::fill_         0.34%      53.863us         4.38%     691.809us     138.362us      30.976us         0.31%      30.976us       6.195us             5  \n",
            "                                             aten::full         0.12%      18.988us         0.90%     142.616us      35.654us       0.000us         0.00%      28.288us       7.072us             4  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      26.144us         0.26%      26.144us       8.715us             3  \n",
            "void splitKreduce_kernel<32, 16, int, float, float, ...         0.00%       0.000us         0.00%       0.000us       0.000us       7.968us         0.08%       7.968us       7.968us             1  \n",
            "                                               aten::to         0.02%       3.266us         0.28%      44.030us      44.030us       0.000us         0.00%       7.680us       7.680us             1  \n",
            "                                         aten::_to_copy         0.04%       6.989us         0.26%      40.764us      40.764us       0.000us         0.00%       7.680us       7.680us             1  \n",
            "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       7.680us         0.08%       7.680us       7.680us             1  \n",
            "                                          aten::__and__         0.02%       3.752us         0.29%      45.571us      22.786us       0.000us         0.00%       7.104us       3.552us             2  \n",
            "                                      aten::bitwise_and         0.17%      27.565us         0.27%      41.819us      20.910us       7.104us         0.07%       7.104us       3.552us             2  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       7.104us         0.07%       7.104us       3.552us             2  \n",
            "                                               aten::gt         0.25%      40.106us         0.36%      56.168us      28.084us       6.976us         0.07%       6.976us       3.488us             2  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.976us         0.07%       6.976us       3.488us             2  \n",
            "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       6.367us         0.06%       6.367us       1.061us             6  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.832us         0.05%       4.832us       2.416us             2  \n",
            "                                             aten::ones         0.06%       9.740us         9.06%       1.430ms       1.430ms       0.000us         0.00%       2.688us       2.688us             1  \n",
            "                                            aten::empty         6.18%     975.034us         6.18%     975.034us      33.622us       0.000us         0.00%       0.000us       0.000us            29  \n",
            "                                       cudaLaunchKernel        12.65%       1.996ms        12.65%       1.996ms      10.080us       0.000us         0.00%       0.000us       0.000us           198  \n",
            "                                    aten::empty_strided         0.06%      10.062us         0.06%      10.062us      10.062us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                       aten::as_strided         1.51%     238.032us         1.51%     238.032us       0.609us       0.000us         0.00%       0.000us       0.000us           391  \n",
            "                                           aten::select         3.46%     546.511us         4.20%     662.735us       3.347us       0.000us         0.00%       0.000us       0.000us           198  \n",
            "                                    cudaPeekAtLastError         0.20%      31.813us         0.20%      31.813us       0.166us       0.000us         0.00%       0.000us       0.000us           192  \n",
            "                                        cudaMemcpyAsync         4.52%     713.064us         4.52%     713.064us       7.428us       0.000us         0.00%       0.000us       0.000us            96  \n",
            "                                  cudaStreamSynchronize        21.10%       3.329ms        21.10%       3.329ms     104.044us       0.000us         0.00%       0.000us       0.000us            32  \n",
            "                                          aten::resize_         0.94%     147.847us         0.94%     147.847us       4.620us       0.000us         0.00%       0.000us       0.000us            32  \n",
            "                                 cudaDeviceGetAttribute         0.15%      23.308us         0.15%      23.308us       0.728us       0.000us         0.00%       0.000us       0.000us            32  \n",
            "                                                aten::t         0.46%      72.891us         1.18%     186.146us       4.540us       0.000us         0.00%       0.000us       0.000us            41  \n",
            "                                        aten::transpose         0.62%      98.489us         1.00%     156.994us       2.754us       0.000us         0.00%       0.000us       0.000us            57  \n",
            "                                             aten::set_         0.50%      78.988us         0.50%      78.988us       2.468us       0.000us         0.00%       0.000us       0.000us            32  \n",
            "                                           aten::unbind         0.54%      85.923us         1.10%     173.380us       5.418us       0.000us         0.00%       0.000us       0.000us            32  \n",
            "                                          aten::reshape         0.52%      81.589us         1.06%     166.593us       2.346us       0.000us         0.00%       0.000us       0.000us            71  \n",
            "                                             aten::view         1.15%     181.656us         1.15%     181.656us       1.747us       0.000us         0.00%       0.000us       0.000us           104  \n",
            "                                           aten::narrow         0.49%      77.800us         1.53%     241.082us       3.767us       0.000us         0.00%       0.000us       0.000us            64  \n",
            "                                            aten::slice         0.91%     142.965us         1.03%     163.282us       2.551us       0.000us         0.00%       0.000us       0.000us            64  \n",
            "                                        aten::unflatten         0.05%       7.606us         0.10%      16.246us       8.123us       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                                        aten::unsqueeze         0.06%       8.698us         0.07%      11.579us       5.790us       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                                          aten::squeeze         0.06%       9.069us         0.07%      10.292us       5.146us       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                                       aten::empty_like         0.05%       8.002us         0.24%      37.589us       9.397us       0.000us         0.00%       0.000us       0.000us             4  \n",
            "                                  cudaStreamIsCapturing         0.02%       2.660us         0.02%       2.660us       1.330us       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                                          aten::permute         0.05%       8.407us         0.06%       9.334us       4.667us       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                                        cudaMemsetAsync         0.29%      45.777us         0.29%      45.777us       7.629us       0.000us         0.00%       0.000us       0.000us             6  \n",
            "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.06%       9.285us         0.06%       9.285us       1.857us       0.000us         0.00%       0.000us       0.000us             5  \n",
            "                                          aten::dropout         0.03%       4.342us         0.03%       4.342us       0.724us       0.000us         0.00%       0.000us       0.000us             6  \n",
            "                                  cudaDeviceSynchronize        18.92%       2.984ms        18.92%       2.984ms       2.984ms       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 15.778ms\n",
            "Self CUDA time total: 9.968ms\n",
            "\n"
          ]
        }
      ]
    }
  ]
}